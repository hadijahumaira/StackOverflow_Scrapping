Number,Title,Question_Text,Answers,Votes,Tags,Link
51,how to set SNOWFLAKE_HOME environment variable and overide default directory path,"I have below python code where i am trying to connect to Snowflake and execute queries in snowflake and generate result as excel file. Currently i am getting error as: UserWarning: Bad owner or permissions on \connections.toml warn(f""Bad owner or permissions on {str(filep)}{chmod_message}"") The snowflake connector always checking the default directory path C:\Users\.snowflake\connections.toml file and i want to change the location of this default directory path and set SNOWFLAKE_HOME environment variable but dont know how to do it. Also i am not sure if there is way so that snowflake connector does not check the connections.toml file connections at all and it should the check the connections defined in python code ? import pandas as pd from sqlalchemy import create_engine import csv import os import warnings from snowflake.sqlalchemy import URL import snowflake.connector # Suppress the specific SAWarning about 'flatten' warnings.filterwarnings(""ignore"", message=""The GenericFunction 'flatten' is already registered"") # Snowflake connection details snowflake_account = 'xxxxxx' snowflake_user = 'xxxxxx' snowflake_password = 'xxxxxx' # URL-encoded password snowflake_database = 'xxxxxx' snowflake_schema = 'xxxxxx' snowflake_warehouse = 'xxxxxx' # Add your warehouse name here # Local paths csv_file_path = r'C:\Users\QueryCSV1.csv' # Path to your local CSV file output_directory = os.path.dirname(csv_file_path) # Use the same directory as CSV file for output ctx = snowflake.connector.connect( user= snowflake_user, password= snowflake_password, account= snowflake_account, warehouse= snowflake_warehouse, database= snowflake_database, schema= snowflake_schema ) engine = create_engine(f'snowflake://{snowflake_user}:{snowflake_password}@{snowflake_account}/{snowflake_database}/{snowflake_schema}?warehouse={snowflake_warehouse}', creator=ctx) # Read queries from the CSV file with UTF-8 encoding to remove BOM with open(csv_file_path, mode='r', encoding='utf-8-sig') as file: reader = csv.reader(file) queries = [row[0] for row in reader if row] # Skip empty rows # Execute each query and save results to separate Excel files for i, query in enumerate(queries): try: # Execute the query using SQLAlchemy engine df = pd.read_sql(query, engine) # Define the output Excel file name output_file = os.path.join(output_directory, f'result_{i + 1}.xlsx') # Save the DataFrame to an Excel file df.to_excel(output_file, index=False) print(f'Saved results of query {i + 1} to {output_file}') except Exception as e: print(f""Error executing query {i + 1}: {e}"") # Close the SQLAlchemy engine connection engine.dispose()",[],0,"python, snowflake-cloud-data-platform, python, snowflake-cloud-data-platform",https://stackoverflow.com/questions/79156454/how-to-set-snowflake-home-environment-variable-and-overide-default-directory-pat
51,How to recreate the iPython console from Pycharm in VScode,"So I'm moving from pycharm to VScode, because a lot of people I work with use jupyter notebooks, and im not going to pay for pycharm professional. For my old workflow, I used to use the IPython console from pycharm. So I would have Ipython as my default interpreter, and every script i was working on, I would change to run in console (which was running in Ipython by default). I could also automatically run scripts everytime i started a new console, and the console had an extremely handy variable view, where I could see exactly which variables were defined, and their shape and stuff. So while coding, I would run each new line in the console first to see if it worked before adding it to the script. Does anyone else work like this? And has anyone else recreated this in VScode? Honestly just an implementation of the dynamic variable view would suffice for me, otherwise working with jupyter notebooks is quite similar to what I was doing.",[],0,"python, visual-studio-code, pycharm, vscode-extensions, python, visual-studio-code, pycharm, vscode-extensions",https://stackoverflow.com/questions/79156418/how-to-recreate-the-ipython-console-from-pycharm-in-vscode
51,How to wait until user input?,User sends some search query through telegram bot. After receiving bot starts parsing website using selenium but website has word captcha. How to catch a moment where site asks to resolve captcha and wait until user input a right word? I know how to wait for input in telegram but didn't find any suggestions for selenium. Maybe should I use a different method?,[],0,"python, selenium-webdriver, html-parsing, user-input, telegram-bot, python, selenium-webdriver, html-parsing, user-input, telegram-bot",https://stackoverflow.com/questions/79156411/how-to-wait-until-user-input
51,DQN model either doesn't work or it is extremely slow in training,"I'm trying to build a DQN model for my PhD progress, and before I implement it with the actual real data, I want to utilize dummy data. Using the same process with simple Q Learning the approach was effective, but once I transitioned it to DQN to make it more advanced and adaptive, I starting facing issues with the training phase. I also implemented GPU acceleration but it doesn't help at all. I wonder if it's because of the size of the dummy dataset, or something else that I can't figure out. Any help or guidance is appreciated. import numpy as np import torch import torch.nn as nn import torch.optim as optim import random import pandas as pd from collections import deque # Set device device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") print(""Running on device:"", device) # Dummy data setup data = { 'message_size': np.random.randint(1000, 70000, size=1000), 'cpu_usage': np.random.uniform(40, 100, size=1000), 'submission_time': np.random.uniform(0, 300, size=1000) } dummy_data = pd.DataFrame(data) # Parameters MAX_BLOCK_SIZE = 32768 ALPHA = 0.1 GAMMA = 0.9 EPSILON = 1.0 EPSILON_MIN = 0.01 EPSILON_DECAY = 0.99 BATCH_SIZE = 32 EPISODES = 1000 # DQN model class DQN(nn.Module): def __init__(self, input_dim): super(DQN, self).__init__() self.fc1 = nn.Linear(input_dim, 64) self.fc2 = nn.Linear(64, 64) self.fc3 = nn.Linear(64, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) return self.fc3(x) # Initialize models and optimizer dqn = DQN(input_dim=2).to(device) target_model = DQN(input_dim=2).to(device) target_model.load_state_dict(dqn.state_dict()) optimizer = optim.Adam(dqn.parameters(), lr=ALPHA) memory = deque(maxlen=2000) # Block choice function def block_choice(state): if random.random() < EPSILON: return random.randint(1, int(state[0] // MAX_BLOCK_SIZE) + 1) else: state_tensor = torch.FloatTensor(state).to(device) return torch.argmax(dqn(state_tensor)).item() + 1 # Reward function based on utility def utility_function_rewarding(total_latency, cpu_per_block, max_latency=300, max_cpu=100): latency_reward = max(0, 1 - (total_latency / max_latency)) cpu_reward = max(0, 1 - (cpu_per_block / max_cpu)) return latency_reward + cpu_reward # Training function def dqn_training(batch_size): if len(memory) < batch_size: return batch = random.sample(memory, batch_size) states, actions, rewards, next_states, dones = zip(*batch) # Move data to device states = torch.FloatTensor(states).to(device) rewards = torch.FloatTensor(rewards).to(device) next_states = torch.FloatTensor(next_states).to(device) dones = torch.FloatTensor(dones).to(device) state_action_values = dqn(states) next_state_values = target_model(next_states).max(1)[0] expected_values = rewards + (GAMMA * next_state_values * (1 - dones)) loss = nn.functional.mse_loss(state_action_values, expected_values) optimizer.zero_grad() loss.backward() optimizer.step() # Store transitions in memory def store_transition(state, action, reward, next_state, done): memory.append((state, action, reward, next_state, done)) # Main training loop for episode in range(EPISODES): print(f""Starting Episode {episode + 1}/{EPISODES}"") row = dummy_data.sample().iloc[0] state = [row['submission_time'], row['cpu_usage']] total_reward = 0 done = False while not done: action = block_choice(state) next_row = dummy_data.sample().iloc[0] next_latency = next_row['submission_time'] next_cpu = next_row['cpu_usage'] / action next_state = [next_latency, next_cpu] reward = utility_function_rewarding(next_latency, next_cpu) total_reward += reward done = episode == EPISODES - 1 store_transition(state, action, reward, next_state, done) state = next_state dqn_training(BATCH_SIZE) # Update epsilon for exploration-exploitation balance if EPSILON > EPSILON_MIN: EPSILON *= EPSILON_DECAY print(f""Episode {episode + 1}/{EPISODES} - Total Reward: {total_reward}"")",[],0,"python, pytorch, reinforcement-learning, dqn, python, pytorch, reinforcement-learning, dqn",https://stackoverflow.com/questions/79156403/dqn-model-either-doesnt-work-or-it-is-extremely-slow-in-training
51,How to generalize the autogenerated Open-Meteo API call to arbitrary weather variables,"I'm using the open-source weather API open-meteo to download weather data. The website provides autogenerated python API calls in the documentation section. I would like to generalize this API call to be able to request arbitrary weather variables using the same code. For a minimal working example, I will use the documentation page mentioned above and set the following values: Location (somewhere in Hamburg): Latitude: 53.33 Longitude: 9.99 Hourly Weather Variables, tick: Temperature (2 m) Wind Speed (10 m) This generates the following API call for downloading the hourly values of the observables temperature_2m (air temperature 2m above the ground) and wind_speed_10m (wind speed 10m above the ground) at a location somewhere in Hamburg. import openmeteo_requests import requests_cache import pandas as pd from retry_requests import retry # Setup the Open-Meteo API client with cache and retry on error cache_session = requests_cache.CachedSession('.cache', expire_after = 3600) retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2) openmeteo = openmeteo_requests.Client(session = retry_session) # Make sure all required weather variables are listed here # The order of variables in hourly or daily is important to assign them correctly below url = ""https://api.open-meteo.com/v1/forecast"" params = { ""latitude"": 53.55, ""longitude"": 9.99, ""hourly"": [""temperature_2m"", ""wind_speed_10m""] } responses = openmeteo.weather_api(url, params=params) # Process first location. Add a for-loop for multiple locations or weather models response = responses[0] print(f""Coordinates {response.Latitude()}°N {response.Longitude()}°E"") print(f""Elevation {response.Elevation()} m asl"") print(f""Timezone {response.Timezone()} {response.TimezoneAbbreviation()}"") print(f""Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s"") # Process hourly data. The order of variables needs to be the same as requested. hourly = response.Hourly() hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy() hourly_wind_speed_10m = hourly.Variables(1).ValuesAsNumpy() hourly_data = {""date"": pd.date_range( start = pd.to_datetime(hourly.Time(), unit = ""s"", utc = True), end = pd.to_datetime(hourly.TimeEnd(), unit = ""s"", utc = True), freq = pd.Timedelta(seconds = hourly.Interval()), inclusive = ""left"" )} hourly_data[""temperature_2m""] = hourly_temperature_2m hourly_data[""wind_speed_10m""] = hourly_wind_speed_10m hourly_dataframe = pd.DataFrame(data = hourly_data) print(hourly_dataframe) My approach so far is to introduce a new python variable custom_variables_list for the names of the weather variables and then loop over it. I skip the part where the ValuesAsNumpy arrays are assigned to new variables only to be then copied to the dictionary. This looks as follows: import openmeteo_requests import requests_cache import pandas as pd from retry_requests import retry custom_variables_list = [""temperature_2m"", ""wind_speed_10m""] # Setup the Open-Meteo API client with cache and retry on error cache_session = requests_cache.CachedSession('.cache', expire_after = 3600) retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2) openmeteo = openmeteo_requests.Client(session = retry_session) # Make sure all required weather variables are listed here # The order of variables in hourly or daily is important to assign them correctly below url = ""https://api.open-meteo.com/v1/forecast"" params = { ""latitude"": 53.55, ""longitude"": 9.99, ""hourly"": custom_variables_list } responses = openmeteo.weather_api(url, params=params) # Process first location. Add a for-loop for multiple locations or weather models response = responses[0] print(f""Coordinates {response.Latitude()}°N {response.Longitude()}°E"") print(f""Elevation {response.Elevation()} m asl"") print(f""Timezone {response.Timezone()} {response.TimezoneAbbreviation()}"") print(f""Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s"") # Process hourly data. The order of variables needs to be the same as requested. hourly = response.Hourly() # hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy() <--- Do not allocate new variables for each observable # hourly_wind_speed_10m = hourly.Variables(1).ValuesAsNumpy() hourly_data = {""date"": pd.date_range( start = pd.to_datetime(hourly.Time(), unit = ""s"", utc = True), end = pd.to_datetime(hourly.TimeEnd(), unit = ""s"", utc = True), freq = pd.Timedelta(seconds = hourly.Interval()), inclusive = ""left"" )} for i, x in enumerate(custom_variables_list): # loop over the list of custom variables hourly_data[x] = hourly.Variables(i).ValuesAsNumpy() # <--- Assign the values here hourly_dataframe = pd.DataFrame(data = hourly_data) print(hourly_dataframe) The code runs without errors and prints the dataframe content. My Question: Is this a valid approach or am I missing some aspects leading to unexpected behavior. In particular, I'm asking since I do not really understand the allocation of individual variables before assigning them to the dictionary in the original API call. I thought about this and I do not see any reason to introduce the extra variables in the original autogenerated call.",[],0,"python, dataframe, weather-api, generalization, python, dataframe, weather-api, generalization",https://stackoverflow.com/questions/79156375/how-to-generalize-the-autogenerated-open-meteo-api-call-to-arbitrary-weather-var
51,Generate arry of positive integers that sum of to k,"My task is simple: I want to generate an (ideally numpy) array containing all combinations of m positive (>=0), but bounded (<= e) integers that sum exactly to k. Note that k and m might be relatively high, so generating all combinations and filtering will not work. I have implemented it in plain, recursive python but this small functions takes most of my time and I need to replace it to perform better. I have tried to come up with numpy/pytorch code to generate this array but I didn't manage to do it so far. I currently use numpy and pytorch in my project, but I am open to other libraries as long as I write python code and I get something I can convert to numpy arrays in the end. Here's some code: import timeit def get_summing_up_to(max_degree, sum, length, current=0): assert sum >= 0 assert length >= 1 if length == 1: residual = sum - current if residual <= max_degree: return [(residual,)] else: return [] max_element = min(max_degree, sum - current) return [ (i,) + t for i in range(max_element + 1) for t in get_summing_up_to( max_degree, sum, length - 1, current=current + i ) ] if __name__ == '__main__': result = timeit.timeit('get_summing_up_to(60, 60, 6)', globals=globals(), number=1) print(f""Execution time: {result} for max_degree=60, sum=60, length=6"") result = timeit.timeit('get_summing_up_to(30, 30, 8)', globals=globals(), number=1) print(f""Execution time: {result} for max_degree=30, sum=30, length=8"")","['[[ANSWER_1], [VOTES=0]] : To solve this problem more efficiently, you can leverage itertools.combinations_with_replacement to generate combinations without needing recursion. This approach should improve performance by reducing function call overhead and allowing for more efficient filtering. import numpy as np import itertools def generate_combinations(max_degree, target_sum, length): result = [] for comb in itertools.combinations_with_replacement(range(1, max_degree + 1), length): if sum(comb) == target_sum: result.append(comb) return np.array(result) if __name__ == ""__main__"": result = generate_combinations(60, 60, 6) print(f""Generated {len(result)} combinations for max_degree=60, sum=60, length=6"") result = generate_combinations(30, 30, 8) print(f""Generated {len(result)} combinations for max_degree=30, sum=30, length=8"")', ""[[ANSWER_2], [VOTES=-1]] : If you want to find all combinations you can use itertools. If you want replacement after each selection use combinations_with_replacement and in that case combinations such as (m,m) will be allowed. Otherwise use combination. import numpy as np import itertools e = 10 # the limit # generate elements to be combined # so they are from 0 to e # with 1 spacing in between each elements = np.linspace(0,e,e+1,dtype=int) # But this can be `Astring`, (tu,ple), ['List', 'of', 'anything'], ... r = 2 # combinations of r elements # Generate all combinations of r elements comb = itertools.combinations_with_replacement(elements, r) # print all the combinations as tuples print(list(comb)) The print shows [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 10), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 10), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 10), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 10), (4, 4), (4, 5), (4, 6), (4, 7), (4, 8), (4, 10), (5, 5), (5, 6), (5, 7), (5, 8), (5, 10), (6, 6), (6, 7), (6, 8), (6, 10), (7, 7), (7, 8), (7, 10), (8, 8), (8, 10), (10, 10)] Then you filter the list to keep only combinations that sums up to k like this k_combs = [] k = 2 # only keeps tuple that sums up to 2 for i,j in enumerate(list(comb)): if np.sum(j) == k: k_combs.append(j) print(k_combs) Which gives [(0, 2), (1, 1)]""]",1,"python, numpy, pytorch, python, numpy, pytorch",https://stackoverflow.com/questions/79156347/generate-arry-of-positive-integers-that-sum-of-to-k
51,Remove words in columns in pandas DataFrame based on total frequency,"I have a dataframe. I want to figure out which words appear exactly five times in the whole corups. I also need to figure out how to remove all of those words from the dataframe/remove the columns. I used this code df.drop([col for col in df.columns if col.endswith('_y')],axis=1,inplace=True) to drop any that ended with y but I don't know how to adjust the code to find words that come up exactly equal to 5.","['[[ANSWER_1], [VOTES=0]] : Assuming words are separated by spaces in your column names... Combine all your column names into a single string and then use count words using str.split() and Counter. List words with a strict count of 5 Create regex of bad words by joining list with \'|\' Identify columns with df.columns.str.contains using regex pattern Drop columns from collections import Counter # Example DataFrame df = pd.DataFrame(columns = [""hello"", ""hello world"", ""hello goodbye"", ""hello hello"", ""foo"", ""foo foo foo foo foo""]) word_count = Counter(\' \'.join(df.columns).split()) remove_words = [word for word in word_count if word_count[word] == 5] columns_to_drop = df.columns[df.columns.str.contains(\'|\'.join(remove_words))] df.drop(columns = columns_to_drop)', ""[[ANSWER_2], [VOTES=0]] : Lets say you have a dataframe: df = pd.DataFrame(data) I have used this dummy example: data = { 'apple': [1, 2, 1], # Total: 4 'banana': [2, 1, 2], # Total: 5 'cherry': [0, 0, 0], # Total: 0 'date': [5, 0, 0], # Total: 5 'elderberry': [3, 2, 1] # Total: 6 } First, calculate the total frequency per word. word_counts = df.sum(axis=0) Second, identify words with exactly five occurances: words_to_remove = word_counts[word_counts == 5].index.tolist() Thirdly, remove the identiifed columns from the dataframe: df.drop(columns=words_to_remove, inplace=True) Print output from my own script: Original DataFrame: apple banana cherry date elderberry 0 1 2 0 5 3 1 2 1 0 0 2 2 1 2 0 0 1 Total frequency of each word: apple 4 banana 5 cherry 0 date 5 elderberry 6 dtype: int64 Words to remove (frequency == 5): ['banana', 'date'] DataFrame after removing words with frequency exactly 5: apple cherry elderberry 0 1 0 3 1 2 0 2 2 1 0 1""]",0,"python, pandas, dataframe, python, pandas, dataframe",https://stackoverflow.com/questions/79156330/remove-words-in-columns-in-pandas-dataframe-based-on-total-frequency
51,Display the address of the segments of an exe file,"I have written a program in Python language that I want to get the name of an exe file and show me the address of the segments related to that file. What is the problem with this code? import pefile def extract_segments(exe_path):try:pe = pefile.PE(exe_path) print(f""\nExtracting segments from: {exe_path}"") print(""="" * 50) for section in pe.sections: name = section.Name.decode('utf-8').strip('\x00') virtual_address = hex(section.VirtualAddress) size = section.SizeOfRawData print(f""Section: {name}"") print(f"" Virtual Address: {virtual_address}"") print(f"" Size: {size} bytes\n"") if name in ['.text', '.data']: print(f"" Content (first 64 bytes):"") print(section.get_data()[:64].hex()) print(""-"" * 50) except FileNotFoundError: print(""Error: File not found."") except pefile.PEFormatError: print(""Error: The file is not a valid PE executable."") if name == ""main"":exe_path = input(""Enter the path of the EXE file: "")extract_segments('sinama.exe')`","['[[ANSWER_1], [VOTES=0]] : As mentioned in the staging area comments, your code is not indented correctly. This is critical in python. Also there were errors in the last 3 correctly indented lines. Below, you will find your code as corrected. import pefile def extract_segments(exe_path): try: pe = pefile.PE(exe_path) print(f""\\nExtracting segments from: {exe_path}"") print(""="" * 50) for section in pe.sections: name = section.Name.decode(\'utf-8\').strip(\'\\x00\') virtual_address = hex(section.VirtualAddress) size = section.SizeOfRawData print(f""Section: {name}"") print(f"" Virtual Address: {virtual_address}"") print(f"" Size: {size} bytes\\n"") if name in [\'.text\', \'.data\']: print(f"" Content (first 64 bytes):"") print(section.get_data()[:64].hex()) print(""-"" * 50) except FileNotFoundError: print(""Error: File not found."") except pefile.PEFormatError: print(""Error: The file is not a valid PE executable."") if __name__ == ""__main__"": exe_path = input(""Enter the path of the EXE file: "") extract_segments(exe_path)']",0,"python, python",https://stackoverflow.com/questions/79156313/display-the-address-of-the-segments-of-an-exe-file
51,"""error"": ""'NoneType' object has no attribute 'seek'"" when trying to decode the message in the file","I've been working on a project that embeds ciphertext into a PNG file using LSB. My lsb_encode function works, the IV is correctly saved in the metadata (I've checked) and overall the file is usable. My main issue is when I try to reverse the process. I get the error in the title using this curl command -> curl -X POST -F ""encoded_image=@C:/Users/PATH/output_image.png"" -F ""delimiter=example delimiter"" ``http://127.0.0.1:5000/png_decode. I have been stuck on this part of the issue for a couple of days now. I am not a developer and this is one of my first 'major' projects I've been working on so any lead is welcome! import logging import numpy as np from PIL import Image, PngImagePlugin, JpegImagePlugin import hashlib from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC from cryptography.hazmat.primitives import hashes, padding from cryptography.hazmat.backends import default_backend from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes import io from io import BytesIO import os from flask import Flask, url_for, jsonify, send_file, request # Code is test only right now. IF it works as intended it should be tuned to adapt to a web service. app = Flask(__name__) logging.basicConfig(level=logging.DEBUG) master_key = 'swlLZ3oalZ5w_sk9ZCgUgmkH7PZLJQqt76D51a5kdAM=' #@app.route('/png_encode', methods=['POST']) def lsb_encoder(file, message, delimiter, iv): # image load and convert to array try: with Image.open(file) as img: if img.mode != 'RGB': img = img.convert('RGB') encoded_image = img.copy() width, height = img.size logging.debug(""Image size: %dx%d"", width, height) #Prepare binary message including delimiter binary_message = ''.join(format(byte, '08b') for byte in message) binary_delimiter = ''.join(format(byte, '08b') for byte in delimiter) binary_message += binary_delimiter logging.debug(""Binary message length in bits: %d"", len(binary_message)) # Check if message fits in image if len(binary_message) > width * height * 3: return None, jsonify({""error"": ""Message cannot be fully hidden in image""}) pixels = encoded_image.load() message_index = 0 # Encode message into image for x in range(width): for y in range(height): if message_index >= len(binary_message): break r, g, b = pixels[x, y] for i in range(3): if message_index >= len(binary_message): break bit = int(binary_message[message_index]) if i == 0: r = (r & ~1) | bit elif i == 1: g = (g & ~1) | bit else: b = (b & ~1) | bit message_index += 1 pixels[x, y] = (r, g, b) output_buffer = io.BytesIO() png_info = PngImagePlugin.PngInfo() png_info.add_text(""iv"", iv.hex()) encoded_image.save(output_buffer, pnginfo=png_info, format=""PNG"", optimize=False) # compress_level=0 If facing image size related errors, set optimize to False output_buffer.seek(0) logging.debug(""Encoded image size after saving %d KB"", output_buffer.tell()//1024) return output_buffer, None except Exception as e: logging.error(""Error during encoding: %s"", e) return None, jsonify({""error"": ""Failed to process and encode image""}) ######################################################################################################################################################################### #@app.route('/png_decode', methods=['POST']) def lsb_decoder(file): try: file.seek(0) with Image.open(file) as img: logging.debug(""Image opened successfully in lsb_decoder."") if img.mode != 'RGB': img = img.convert('RGB') width, height = img.size logging.debug(""Encoded image size: %dx%d"", width, height) pixels = img.load() binary_message = '' # Extract LSB bits to form binary string for x in range(width): for y in range(height): r, g, b = pixels[x, y] binary_message += str(r & 1) binary_message += str(g & 1) binary_message += str(b & 1) # Convert binary string to bytes binary_message = [binary_message[i:i+8] for i in range(0, len(binary_message), 8)] message_bytes = bytearray() for byte in binary_message: if len(byte) < 8: continue # Skip if not a full byte message_bytes.append(int(byte, 2)) # Return the extracted bytes (ciphertext + delimiter), without IV return bytes(message_bytes), None except Exception as e: logging.error(""Error during extraction: %s"", e) return None, str(e) # return jsonify({""message"": decoded_message}), 200 @app.route('/png_encode', methods=['POST']) def data_preprocessing(): file = request.files.get('image') #get image from upload message = request.form.get('message') #get message to be stegoed into image delimiter = request.form.get('delimiter') #get delimiter to be hashed for derived key and stegoed in the image if not file or not message or not delimiter: return jsonify({""error"": ""Image, message and delimiter are required""}), 400 hashed_delimiter = hashlib.sha256(delimiter.encode('utf8')).digest() #hash delimiter iv = os.urandom(16) kdf = PBKDF2HMAC( algorithm=hashes.SHA256(), length=32, #32 bytes = 256 bit key for AES-256 salt = hashed_delimiter, iterations=10000, backend=default_backend() )#derive key for encryption of message derived_key = kdf.derive(master_key.encode('utf8')) message_bytes = message.encode('utf8') #AES cipher in CBC mode cipher = Cipher(algorithms.AES(derived_key), modes.CBC(iv), backend=default_backend()) #Padding to make it multiple of 16 bytes padder = padding.PKCS7(algorithms.AES.block_size).padder() padded_message = padder.update(message_bytes) + padder.finalize() #Encrypt the padded message encryptor = cipher.encryptor() ciphertext = encryptor.update(padded_message) + encryptor.finalize() encoded_image, error_response = lsb_encoder(file, ciphertext, hashed_delimiter, iv) #error_response if error_response: return error_response return send_file(encoded_image, mimetype=""image/png"", as_attachment=True, download_name=""encoded_image.png"") @app.route('/png_decode', methods=['POST']) def data_deciphering(): encoded_image = request.files.get('encoded_image') # Get the encoded image from upload delimiter = request.form.get('delimiter') # Get the delimiter used for hashing if not encoded_image or not delimiter: return jsonify({""error"": ""Image and delimiter are required""}), 400 hashed_delimiter = hashlib.sha256(delimiter.encode('utf8')).digest() # Hash the delimiter # Decode the image to extract the ciphertext and delimiter (IV not included here) ciphertext_with_delimiter, error_response = lsb_decoder(encoded_image) if error_response: return jsonify({""error"": error_response}), 400 # Extract IV from image metadata in data_deciphering try: with Image.open(encoded_image) as img: iv_hex = img.info.get(""iv"", None) if iv_hex is None: return jsonify({""error"": ""IV not found in image metadata""}), 400 iv = bytes.fromhex(iv_hex) except Exception as e: logging.error(""Error reading IV from image metadata: %s"", e) return jsonify({""error"": ""Failed to extract IV from image""}), 400 # Derive key for decryption kdf = PBKDF2HMAC( algorithm=hashes.SHA256(), length=32, # 32 bytes = 256 bit key for AES-256 salt=hashed_delimiter, iterations=10000, backend=default_backend() ) derived_key = kdf.derive(master_key.encode('utf8')) # AES cipher in CBC mode for decryption cipher = Cipher(algorithms.AES(derived_key), modes.CBC(iv), backend=default_backend()) decryptor = cipher.decryptor() # Decrypt the ciphertext padded_message = decryptor.update(ciphertext_with_delimiter) + decryptor.finalize() # Unpadding to get the original message unpadder = padding.PKCS7(algorithms.AES.block_size).unpadder() original_message = unpadder.update(padded_message) + unpadder.finalize() message_length = len(original_message) - 32 original_message = original_message[:message_length].decode('utf8') return jsonify({""message"": original_message}), 200 ######################################################################################################################################################################### if __name__ == '__main__': app.run(debug=True) Added several logging statements to try and track where the process fails but to no avail.",[],0,"python, flask, cryptography, steganography, hashlib, python, flask, cryptography, steganography, hashlib",https://stackoverflow.com/questions/79156260/error-nonetype-object-has-no-attribute-seek-when-trying-to-decode-the-m
51,"When a Flask server receives a request, it makes a GET request in return [closed]","Closed. This question needs debugging details. It is not currently accepting answers. Edit the question to include desired behavior, a specific problem or error, and the shortest code necessary to reproduce the problem. This will help others answer the question. Closed 7 mins ago. Improve this question When accessing 0.0.0.0:5000 on the server side, received is displayed, but on the client side, the data sent from the server is not being saved. It's likely that the code either incorrectly sends the data from the server, or the client's code data = response.json() save_file=""received_data.txt"" #location with open(save_file, 'w') as file: for value in data: file.write(f'{value}\n') is incorrect client incorrectly processes the data, but I'm not sure what to fix. #client code from flask import Flask, request import requests import json import logging app = Flask(__name__) logging.basicConfig(filename='data.log', level=logging.INFO) get_request = 0 @app.route('/') def get_coordinate(): try: response = requests.get(""http://0.0.0.0:5000"") if response.status_code == 200: data = response.json() save_file=""received_data.txt"" #location with open(save_file, 'w') as file: for value in data: file.write(f'{value}\n') else: print(f""Failed to fetch data, status code: {response.status_code}"") except requests.exceptions.RequestException as e: print(f""Request failed: {e}"") return ""received"", 200 if __name__ == '__main__': app.run(debug=True, host='0.0.0.0', port=5000) #server code from flask import Flask, jsonify app = Flask(__name__) @app.route('/', methods=['GET']) def send_data(): data = [12.34, 56.78, 90.12, 34.56] return jsonify(data) if __name__ == '__main__': app.run(host='0.0.0.0', port=5000) I connected using the same Wi-Fi and communicated using the internal IP. and I also not 0.0.0.0, but actually using the server's IP. Logging doesn't seem necessary, but it was added for verification purposes, so it's okay to ignore it.",[],-1,"python, flask, python-requests, python, flask, python-requests",https://stackoverflow.com/questions/79156244/when-a-flask-server-receives-a-request-it-makes-a-get-request-in-return
51,MongoDb throws out an error saying bas auth,"I have been trying to connect mongodb to my python code and everytime I try to run, it says ""Error: bad auth : authentication failed, full error: {'ok': 0, 'errmsg': 'bad auth : authentication failed', 'code': 8000, 'codeName': 'AtlasError'}"". I have tried everything that I can think of, checked the password, all the permissions and stuff but all in vain. Would appreciate it if someone guides on how to solve it. I am using python 3.8 I tried changing the password, but no. I tried assigning the password to a variable and then using it while connecting to the client, nope. I tried using pymongo library to connect to mongoDB client but still no.",[],-1,"python, database, mongodb, pymongo, python, database, mongodb, pymongo",https://stackoverflow.com/questions/79156241/mongodb-throws-out-an-error-saying-bas-auth
51,How do I turn-off this horizontal line in the FFT plot,"I am trying to show the trend in observation data and the corresponding fft . A horizontal line keeps appearing in the fft, part that I do not need to show in the chart. I give a MWE (pseudo data) below. import numpy as np from scipy.fft import fft, fftfreq import matplotlib.pyplot as plt observations = np.array([ 3.78998207e-01, 3.05199629e-01, 2.29614343e-01, 1.86568613e-01, 1.83449462e-01, 1.77892746e-01, 1.66237352e-01, 1.81950778e-01, 9.88351226e-02, 1.29674430e-01, 7.08703360e-02, 3.64963487e-02, 2.75641060e-02, 6.21573753e-02, 8.51043646e-02, 5.32184940e-02, 6.47005530e-02, -6.41628893e-02, -1.86618020e-01, -4.08624200e-02, -2.71649960e-02, -8.22041576e-03, 9.13242105e-03, 1.67080717e-01, -1.37465317e-01, 2.74977101e-04, 4.47602122e-02, 8.27649668e-02, -5.60661808e-02, -2.26248880e-01, -1.54768403e-01, -4.46428484e-02, 4.57611677e-02, 9.83215698e-02, 9.22357256e-02, -1.23436114e-01, -2.76981909e-01, -1.98824586e-01, -2.33452893e-01, -2.57550630e-01, -9.13919527e-02, 2.64029442e-02, -5.44394568e-02, 4.02010984e-01, 3.27256645e-01, 2.14259077e-01, 5.08021357e-01, 5.55141121e-01, 6.11203693e-01, 5.34086779e-01, 2.19652659e-01, 1.71635054e-01, 1.30867565e-01, 1.25133212e-01, 1.02010973e-01, 1.16727950e-02, 2.84545455e-02, -1.73553706e-02, -1.33998184e-01, -1.36456573e-01, -1.68706794e-01, -1.28378379e-01, -1.43710423e-01, -2.02454545e-01, -4.30164457e-01, -5.19982175e-01, -3.74452537e-01, -3.64076796e-01, -3.20950700e-01, -2.34052515e-01, -1.37158482e-01, 2.80797054e-02, 7.04379682e-02, 1.13920696e-01, 1.26391389e-01, 9.31688808e-02, 1.46000000e-01, 1.18380338e-01, 5.18909438e-02, 1.11584791e-01, 6.43582617e-02, -6.36856386e-02, -9.16134931e-02, -1.02616820e-01, -4.43179890e-01, -1.28223431e+00, -1.86160058e+00, -1.43772912e+00, -1.21047880e+00, -7.21282278e-01, -1.65349241e-01, 4.58791266e-02, 2.42897190e-01, 3.26587994e-01, 3.15827382e-01, -5.29090909e-02, 8.97887313e-03, 2.61194000e-02, -2.24566234e-01, -9.18572710e-02]) observed_fft = fft(observations) fs = 100 n = observations.size fft_fre = fftfreq(n, d=1/fs) x_time = np.arange(len(observations)) fig, axs = plt.subplots(2) axs[0].plot(x_time, observations) axs[1].plot(fft_fre, np.abs(observed_fft)) Output: I know this horizontal line appears because the FFT frequencies in fft_fre include both positive and negative frequencies, thus a symmetrical plot around zero frequency (and I needed to show both the +ve and ve frequencies). But isn't there a workaround to turn-off the line connecting the last negative frequency to the first positive frequency?",[],0,"python, numpy, matplotlib, fft, python, numpy, matplotlib, fft",https://stackoverflow.com/questions/79156239/how-do-i-turn-off-this-horizontal-line-in-the-fft-plot
51,How do I make bi-directional bar chart that breaks down the P/L from each bet outcome vs the outcome?,"I have python code where I create code that graphs each betting scenarios. My code returns this [graph ](graph) import pandas as pd import matplotlib.pyplot as plt import itertools bet_details = { ""Team 1"": {""bet_amount"": 1000, ""price_per_share"": 0.37, ""bet_position"": ""L""}, ""Team 2"": {""bet_amount"": 1000, ""price_per_share"": 0.59, ""bet_position"": ""W""}} # ""L"" for Lose, ""W"" for Win # Calculate P/L based on prediction market share model def calculate_profit(bet_amount, odds, position, outcome): # Calculate the number of shares based on price per share shares = bet_amount / odds if position == outcome: return shares # If bet is correct, return $1 per share else: return -bet_amount # If bet is incorrect, lose the bet amount # Function to simulate all scenarios and calculate P/L for each one def simulate_bets(bet_details): # Generate all possible win/loss combinations for the given states scenarios = list(itertools.product([""W"", ""L""], repeat=len(bet_details))) # Initialize lists to store results scenario_results = [] breakdown = [] # Calculate P/L for each scenario for scenario in scenarios: scenario_name = "" & "".join([f""{state}: {'Win' if outcome == 'W' else 'Lose'}"" for state, outcome in zip(bet_details.keys(), scenario)]) total_profit_loss = 0 # Breakdown details for each state in this scenario state_breakdown = [] # Calculate P/L for each state in the scenario for state, outcome in zip(bet_details.keys(), scenario): details = bet_details[state] bet_amount = details[""bet_amount""] odds = details[""odds""] position = details[""bet_position""] # Calculate the profit/loss for this state in the current scenario profit_loss = calculate_profit(bet_amount, odds, position, outcome) total_profit_loss += profit_loss # Append breakdown information state_breakdown.append({ ""State"": state, ""Bet Amount"": bet_amount, ""Bet Position"": position, ""Bet Outcome"": outcome, ""P/L"": profit_loss }) # Add scenario results to main results scenario_results.append({""Scenario"": scenario_name, ""Total Profit/Loss"": total_profit_loss}) breakdown.extend(state_breakdown) # Create DataFrames for scenario overview and detailed breakdown df_scenarios = pd.DataFrame(scenario_results).sort_values(by=""Total Profit/Loss"", ascending=False) df_breakdown = pd.DataFrame(breakdown) # Plotting the scenario results plot_scenarios(df_scenarios) return df_scenarios, df_breakdown def plot_scenarios(df_scenarios): fig, ax = plt.subplots(figsize=(10, 8)) bars = ax.barh(df_scenarios['Scenario'], df_scenarios['Total Profit/Loss'], color=[""green"" if x > 0 else ""red"" for x in df_scenarios['Total Profit/Loss']]) # Add labels ax.set_xlabel(""Total Profit/Loss ($)"") ax.set_title(""Profit/Loss by Scenario"") # Adding text labels for P/L values with alignment adjustment for bar in bars: width = bar.get_width() ax.text(width + (10 if width < 0 else -40), bar.get_y() + bar.get_height() / 2, f""{width:.2f}"", va=""center"", ha=""left"" if width < 0 else ""right"", color=""black"", fontweight=""bold"") plt.show() Is there a way to create graph that shows the break down of each bet P/L on a bar opposite of each other and shows the bet name and P/L. When I tried the best I could do is this messy [graph](graph). import pandas as pd import matplotlib.pyplot as plt import itertools # Define the bet details for each team in a dictionary bet_details = { ""Team 1"": {""bet_amount"": 100, ""odds"": 0.37, ""bet_position"": ""L""}, # ""L"" for Lose, ""W"" for Win ""Team 2"": {""bet_amount"": 100, ""odds"": 0.59, ""bet_position"": ""W""}, } # Calculate P/L based on prediction market share model def calculate_profit(bet_amount, odds, position, outcome): # Calculate the number of shares based on price per share shares = bet_amount / odds if position == outcome: return shares # If bet is correct, return $1 per share else: return -bet_amount # If bet is incorrect, lose the bet amount # Function to simulate all scenarios and calculate P/L for each one def simulate_bets(bet_details): # Generate all possible win/loss combinations for the given teams scenarios = list(itertools.product([""W"", ""L""], repeat=len(bet_details))) # Initialize lists to store results scenario_results = [] breakdown = [] # Calculate P/L for each scenario for scenario in scenarios: scenario_name = "" & "".join([f""{team}: {'Win' if outcome == 'W' else 'Lose'}"" for team, outcome in zip(bet_details.keys(), scenario)]) total_profit_loss = 0 # Breakdown details for each team in this scenario team_breakdown = [] # Calculate P/L for each team in the scenario for team, outcome in zip(bet_details.keys(), scenario): details = bet_details[team] bet_amount = details[""bet_amount""] odds = details[""odds""] position = details[""bet_position""] # Calculate the profit/loss for this team in the current scenario profit_loss = calculate_profit(bet_amount, odds, position, outcome) total_profit_loss += profit_loss # Append breakdown information team_breakdown.append({ ""Team"": team, ""Bet Amount"": bet_amount, ""Bet Position"": position, ""Bet Outcome"": outcome, ""P/L"": profit_loss }) # Add scenario results to main results scenario_results.append({""Scenario"": scenario_name, ""Total Profit/Loss"": total_profit_loss}) breakdown.extend(team_breakdown) # Create DataFrames for scenario overview and detailed breakdown df_scenarios = pd.DataFrame(scenario_results).sort_values(by=""Total Profit/Loss"", ascending=False) df_breakdown = pd.DataFrame(breakdown) # Plotting the scenario results plot_scenarios(df_scenarios, df_breakdown) return df_scenarios, df_breakdown def plot_scenarios(df_scenarios, df_breakdown): fig, ax = plt.subplots(figsize=(12, 8)) # Main scenario profit/loss bars bars = ax.barh(df_scenarios['Scenario'], df_scenarios['Total Profit/Loss'], color=[""green"" if x > 0 else ""red"" for x in df_scenarios['Total Profit/Loss']], edgecolor='black') # Adding text labels for scenario P/L values for bar in bars: width = bar.get_width() ax.text(width + (10 if width < 0 else -40), bar.get_y() + bar.get_height() / 2, f""{width:.2f}"", va=""center"", ha=""left"" if width < 0 else ""right"", color=""black"", fontweight=""bold"") # Create a mirrored bar for team-level P/L breakdown for index, scenario in enumerate(df_scenarios['Scenario']): # Get team breakdown for this scenario team_data = df_breakdown[df_breakdown['Team'].isin(bet_details.keys())] # Calculate the profit/loss for each team in this scenario for team in bet_details.keys(): details = bet_details[team] profit_loss = calculate_profit(details['bet_amount'], details['odds'], details['bet_position'], ""W"" if df_scenarios.iloc[index]['Total Profit/Loss'] > 0 else ""L"") # Create a bar for the team's P/L on the opposite side ax.barh(scenario, profit_loss, left=-profit_loss, color='lightblue', edgecolor='black', height=0.4) # Add team P/L labels on the opposite side ax.text(-profit_loss - 5, index, f""{team}: {profit_loss:.2f}"", va='center', color='black', fontweight='bold') # Set labels and title ax.set_xlabel(""Total Profit/Loss ($)"") ax.set_title(""Profit/Loss by Scenario with Team Breakdown"") plt.show() # Example usage: df_results, df_breakdown = simulate_bets(bet_details) print(df_results) # Scenario summary with total P/L print(df_breakdown) # Detailed P/L breakdown by team",[],0,"python, pandas, matplotlib, plot, plotly, python, pandas, matplotlib, plot, plotly",https://stackoverflow.com/questions/79156224/how-do-i-make-bi-directional-bar-chart-that-breaks-down-the-p-l-from-each-bet-ou
51,How often are floats cast to ints in PyTorch?,"One question I had was regarding casting, specifically how often are floats cast to ints? Casting an int to a float for an operation like mean seems reasonable to me, however I can't see an instance where going the other direction makes sense, unless there is some level of memory being saved? So I guess my questions are: Generally speaking, are floats cast to ints very often? Do ints provide less computational cost than floats in operations?",[],0,"python, pytorch, python, pytorch",https://stackoverflow.com/questions/79156215/how-often-are-floats-cast-to-ints-in-pytorch
51,"Convert pandas series of strings into PeriodIndex, quarterly","I have a column of strings which represent quarters (format '%Y%q') which I want to convert to PeriodIndex. How to do this? Nothing is working, can't find a strptime function that takes a format string and deals with Quarterly. out[0] 0 200001 1 200002 2 200003 3 200004 4 200101 ... 94 202303 95 202304 96 202401 97 202402 98 202403 Name: 0, Length: 99, dtype: int64 Want: PeriodIndex(['2000Q1',... '2024Q3'], dtype='period[Q-DEC]')","[""[[ANSWER_1], [VOTES=0]] : Regex replace Firstly, what you're showing isn't string data, it's dtype: int64. I'm going to assume you're reading from a CSV or something and you can go back and fix that. You can simply use regex to convert to the right format. Note that %q doesn't have a leading zero (per docs for pandas.Period.strftime). # Example setup s = pd.Series(['202304', '202401', '202402', '202403']) pd.PeriodIndex( s.str.replace(r'(\\d{4})0(\\d)', r'\\1Q\\2', regex=True), freq='Q') PeriodIndex(['2023Q4', '2024Q1', '2024Q2', '2024Q3'], dtype='period[Q-DEC]') Regex breakdown: \\d A digit {4} Repeated four times (...) A group \\1 Reference back to group 1"", ""[[ANSWER_2], [VOTES=0]] : You can use pd.PeriodIndex.from_fields. With integers (you say you have strings, but your sample has dtype: int64), use floor division for the year (//), and modulo for the quarter (%): import pandas as pd data = [202401, 202402, 202403, 202404] df = pd.DataFrame({'Dates': data}) df['Quarters'] = pd.PeriodIndex.from_fields(year=df['Dates'] // 100, quarter=df['Dates'] % 10, freq='Q') Output: df['Quarters'] 0 2024Q1 1 2024Q2 2 2024Q3 3 2024Q4 Name: Quarters, dtype: period[Q-DEC] With strings, you can use Series.astype to convert to integers and do the same as above, or slice via Series.str and convert afterwards: df = pd.DataFrame({'Dates': data}, dtype=str) df['Quarters'] = pd.PeriodIndex.from_fields(year=df['Dates'].str[:4].astype(int), quarter=df['Dates'].str[5:].astype(int), freq='Q') Note that the use of pd.PeriodIndex in this way is deprecated since version 2.2.0 (e.g. pd.PeriodIndex(year=[2024], quarter=[1])). Performance comparison Adding the pd.PeriodIndex method used in the answer by @wjandrea: s = pd.Series(['202401', '202402', '202403', '202404']) s = pd.concat([s]*250) %timeit pd.PeriodIndex.from_fields(year=s.str[:4].astype(int), quarter=s.str[5:].astype(int), freq='Q') 1.65 ms ± 273 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each) %timeit pd.PeriodIndex(s.str.replace(r'(\\d{4})0(\\d)', r'\\1Q\\2', regex=True), freq='Q') 21.9 ms ± 833 μs per loop (mean ± std. dev. of 7 runs, 10 loops each) # alternative for replace with regex %timeit pd.PeriodIndex(s.str[:4] + 'Q' + s.str[5:], freq='Q') 21.3 ms ± 651 μs per loop (mean ± std. dev. of 7 runs, 10 loops each) Using pd.PeriodIndex.from_fields will be much faster. Used directly on integers it will be faster still: s = pd.Series([202401, 202402, 202403, 202404]) s = pd.concat([s]*250) %timeit pd.PeriodIndex.from_fields(year=s // 100, quarter=s % 10, freq='Q') 1.05 ms ± 142 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)""]",0,"python, pandas, time-series, period, python, pandas, time-series, period",https://stackoverflow.com/questions/79156209/convert-pandas-series-of-strings-into-periodindex-quarterly
51,I cant click the heart button with Shadow DOM,"i try to program a like bot. but i cant click the button. I tried my best, can you help me?enter image description hereenter image description here can someone look after my screenshots and and the script and help me please? i dont know what to do. i will login, open the profil but dont like the picture. from selenium import webdriver from selenium.webdriver.chrome.options import Options from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.webdriver.common.keys import Keys import time # Account credentials accounts = [ {""username"": ""......"", ""password"": "".......""} ] def login_and_click_photo_card(driver, username, password): print(""Navigating to login page..."") driver.get('https://4based.com/login') try: # Wait for the email input field to be visible and type the email print(""Waiting for the email input field..."") email_field = WebDriverWait(driver, 30).until( EC.visibility_of_element_located((By.NAME, 'email')) ) email_field.click() email_field.clear() email_field.send_keys(username) print(""Email entered successfully."") # Wait for the password input field to be visible and type the password print(""Waiting for the password input field..."") password_field = WebDriverWait(driver, 30).until( EC.visibility_of_element_located((By.NAME, 'password')) ) password_field.click() password_field.clear() password_field.send_keys(password) print(""Password entered successfully."") # Press ""Enter"" to submit the form print(""Pressing Enter to submit the form..."") password_field.send_keys(Keys.RETURN) print(""Enter key pressed successfully."") # Wait for the ""Nein danke"" button and click it print(""Waiting for the 'Nein danke' button..."") nein_danke_button = WebDriverWait(driver, 30).until( EC.element_to_be_clickable((By.XPATH, ""//ion-button[contains(., 'Nein danke')]"")) ) nein_danke_button.click() print(""'Nein danke' button clicked successfully."") # Navigate directly to the profile page print(""Navigating to the profile page..."") driver.get('https://4based.com/profile/stellasolamente') print(""Successfully navigated to the profile page."") # Wait for the first ""file-stack-card"" element and click it print(""Waiting for the first 'file-stack-card' element to be clickable..."") first_photo_card = WebDriverWait(driver, 30).until( EC.element_to_be_clickable((By.CSS_SELECTOR, ""file-stack-card.virtual-item"")) ) first_photo_card.click() print(""First 'file-stack-card' clicked successfully."") # Wait for 2 seconds to observe the result time.sleep(2) print(""Waited for 2 seconds after clicking the photo card."") # Use JavaScript to scroll the window by 200 pixels print(""Attempting to scroll down the window by 200 pixels..."") driver.execute_script(""window.scrollBy(0, 200);"") print(""Scroll down by 200 pixels completed."") # Access the Heart button in the Shadow DOM print(""Accessing the Heart button in the Shadow DOM..."") # Find the outer shadow host element shadow_host = driver.find_element(By.CSS_SELECTOR, ""ion-button[name='heart-outline']"") shadow_root = driver.execute_script(""return arguments[0].shadowRoot"", shadow_host) # Traverse deeper into the Shadow DOM if needed icon_wrapper = shadow_root.find_element(By.CSS_SELECTOR, ""div.icon-inner"") heart_icon = icon_wrapper.find_element(By.CSS_SELECTOR, ""svg"") # Click the heart icon using JavaScript driver.execute_script(""arguments[0].click();"", heart_icon) print(""Heart button clicked successfully using JavaScript."") except Exception as e: print(f""An error occurred: {e}"") time.sleep(30) if __name__ == ""__main__"": login_and_click_photo_card(webdriver.Chrome(), '[email protected]', 'password') can someone help me with the code?",[],0,"python, selenium-webdriver, python, selenium-webdriver",https://stackoverflow.com/questions/79156205/i-cant-click-the-heart-button-with-shadow-dom
51,Kivy's Label goes Blank after handling too much data,"I'm new to both Kivy and Python and we have a Label that is exceeding texture size in ScrollView when a new flow of data is fed to it upon scan triggered by a Button. Let's say in the Pixel 7 emulator, when texture size of Label on y axis exceeds hardware's texture size limit: 8192 that Label goes blank/black but it is still possible to scroll through but nothing is visible apart from the top information e.g., Buttons or other Widgets. I've been looking through the internet hoping to find an answer however, it is a pointless search. Some users refer to RecycleView but Kivy documentation does not make it easy implementing it and from my understanding RV is designed to handle bigger influx of data and is more memory efficient. Question is, does RV also help with texture size limitations? I came across an article (cannot find link to it) which led me reading up on ScrollLabel but the repository on Git seems dead.",[],0,"python, kivy, python, kivy",https://stackoverflow.com/questions/79156203/kivys-label-goes-blank-after-handling-too-much-data
51,Is there an analogous way to authenticate to `pyodbc.connect('...;Authentication=ActiveDirectoryIntegrated')` for working with SharePoint/OneDrive?,"I am able to connect to my company's Azure Synapse instance using code like the following: server = 'our server' database = 'our database' driver = '{ODBC Driver 17 for SQL Server}' authentication = 'ActiveDirectoryIntegrated' with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';DATABASE='+database+';Authentication='+authentication) as conn: ... The code runs and authenticates without any popup to sign-in and allows me to query whatever I need. What I am wanting is some way of interacting with Office 365/SharePoint/OneDrive via Python that is similar. I've had other use cases in the past that I've found work arounds with Power Automate or otherwise. My current use case is being able to save emails from Outlook as .msg files. I found two solutions that I think I can combine to do that: Reading e-mails from Outlook with Python through MAPI Saving to .msg file in Python, or alternatively, sending mail to the file system The problem is, when I run the following, it doesn't grab the recent emails. My guess is it's grabbing emails from when I started my computer or Outlook. inbox = outlook.GetDefaultFolder(6) messages = inbox.Items message = messages.GetFirst() for i in range(10): next_msg = messages.GetNext() if ""trigger text"" in next_msg.body.lower(): next_msg.SaveAs(r""my save path"", 3) I'm trying to avoid having to register a web app in Azure because I'm not part of my company's IT org, and they make it basically impossible to get approved. I'm fine with it if it can only run when I'm at my computer and logged in, though it would be nice if it could run regardless. I'm open to a solution via Power Automate, but if I could somehow get a token or some way to authenticate to Azure/O365 via Python, I feel like I could streamline a lot of things I do instead of making a web of Power Automate and other systems to get the job done.",[],0,"python, authentication, outlook, active-directory, office365, python, authentication, outlook, active-directory, office365",https://stackoverflow.com/questions/79156201/is-there-an-analogous-way-to-authenticate-to-pyodbc-connect-authentication
51,How to overcome Python3 installation Fatal Python error: init_fs_encoding,"While attempting to troubleshoot an issue on my Ubuntu for Windows installation I uninstalled all the python instances. The issue has been resolved, but now I'm unable to get python3 reinstalled. $ sudo apt install python3 Reading package lists... Done Building dependency tree... Done Reading state information... Done The following additional packages will be installed: libpython3-stdlib python3-minimal python3.10 Suggested packages: python3-doc python3-tk python3-venv python3.10-venv python3.10-doc The following NEW packages will be installed: libpython3-stdlib python3 python3-minimal python3.10 0 upgraded, 4 newly installed, 0 to remove and 35 not upgraded. 1 not fully installed or removed. Need to get 0 B/562 kB of archives. After this operation, 905 kB of additional disk space will be used. Do you want to continue? [Y/n] y Setting up python3.10-minimal (3.10.12-1~22.04.6) ... # Empty sitecustomize.py to avoid a dangling symlink Could not find platform independent libraries <prefix> Could not find platform dependent libraries <exec_prefix> Consider setting $PYTHONHOME to <prefix>[:<exec_prefix>] Python path configuration: PYTHONHOME = (not set) PYTHONPATH = (not set) program name = '/usr/bin/python3.10' isolated = 0 environment = 0 user site = 1 import site = 0 sys._base_executable = '/usr/bin/python3.10' sys.base_prefix = '/usr' sys.base_exec_prefix = '/usr' sys.platlibdir = 'lib' sys.executable = '/usr/bin/python3.10' sys.prefix = '/usr' sys.exec_prefix = '/usr' sys.path = [ '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/lib-dynload', ] Fatal Python error: init_fs_encoding: failed to get the Python codec of the filesystem encoding Python runtime state: core initialized ModuleNotFoundError: No module named 'encodings' Current thread 0x00007fcf47778000 (most recent call first): <no Python frame> dpkg: error processing package python3.10-minimal (--configure): installed python3.10-minimal package post-installation script subprocess returned error exit status 1 Errors were encountered while processing: python3.10-minimal E: Sub-process /usr/bin/dpkg returned an error code (1) I'm not certain, but I think this may be the core error: Fatal Python error: init_fs_encoding: failed to get the Python codec of the filesystem encoding Python runtime state: core initialized ModuleNotFoundError: No module named 'encodings' Any ideas?",[],1,"python, python-3.x, ubuntu, installation, windows-subsystem-for-linux, python, python-3.x, ubuntu, installation, windows-subsystem-for-linux",https://stackoverflow.com/questions/79156198/how-to-overcome-python3-installation-fatal-python-error-init-fs-encoding
51,Keras branch model issue,"I have constructed the following CNN model: def create_model(input_shape=(128, 128, 3)): inputs = keras.Input(shape=input_shape, name='inputs') # main block main_block = keras.Sequential([ bunch of layers ], name='main_block') main_block_outputs = main_block(inputs) # branch 1: black branch_black = keras.Sequential([ bunch of layers ], name='black') branch_black_outputs = branch_black(main_block_outputs) # branch 2: square branch_square = keras.Sequential([ bunch of layers ], name='square') branch_square_outputs = branch_square(main_block_outputs) return keras.Model(inputs=inputs, outputs=[branch_black_outputs, branch_square_outputs], name='model') model = create_model() # expand model architecture to show all layers model.summary(expand_nested=True) plot_model(model, show_shapes=True, expand_nested=True, show_layer_activations=True) when I plot this model though, I can see the two branches but they are both called ""Sequential"". Worse still when I compile it and run it using the following. I get summerised values, not values broken down by branch. I know I am doing something basic wrong but I cant see to work it out... model.compile( optimizer=optimizers.Adam(learning_rate=4e-3), loss={ 'black': losses.MeanAbsoluteError(), 'square': losses.BinaryCrossentropy() }, metrics={ 'black': metrics.MeanAbsoluteError(), 'square': metrics.BinaryAccuracy() } ) Appreciate your patience, I new to this. I was expecting a plot with named branches ""black"" and ""square"" and metric for each branch once run.",[],0,"python, keras, python, keras",https://stackoverflow.com/questions/79156190/keras-branch-model-issue
51,Locale-aware number verification in Python?,"I need to verify a string has a valid number format as opposed to a date (and random strings containing numbers) I tried parsing the string using locale.atof(), expecting a ValueError on invalid inputs. import locale locale.setlocale(locale.LC_ALL, 'de_DE.UTF-8') valid = locale.atof('1.234.567,89') invalid = locale.atof('31.10.2024') Unfortunately, it appears like locale.atof() just removes any thousands separator, regardless of their position, so invalid is parsed to 31102024.0. Is there a pythonic way to do locale-aware number parsing that doesn't accept mis-formated numbers?",[],1,"python, internationalization, python, internationalization",https://stackoverflow.com/questions/79156152/locale-aware-number-verification-in-python
51,How do I convert a 2D coordinates to 3D real world coordinates using open3d python?,"I don't want to create a 3D model. I just want to convert a given point in an image to its corresponding 3D coordinate. While researching I found deep learning models that calculate the depth which will be useful to find the 3D coordinates. But I am understand how to access the depth values after using the DL model on the video. The video is being generated and saved but I don't know where to access the depth values. After that I came across open3D which has an method which can help convert a single point to a 3D point but it requires ""depth"", ""intrinsic parameters"" and ""extrinsic parameters"". open3d.geometry.create_point_cloud_from_depth_image(depth, intrinsic) My question is: How do I find intrinsic and extrinsic parameters using open3D or any other way? Apart from open3D or anything, how do I find instrinsic, extrinsic and depth values in any other way? Are there any good books/tutorials which explains this?",[],0,"python, computer-vision, coordinates, depth, open3d, python, computer-vision, coordinates, depth, open3d",https://stackoverflow.com/questions/79156147/how-do-i-convert-a-2d-coordinates-to-3d-real-world-coordinates-using-open3d-pyth
51,WxPython Label catches focus on start,"I don't know whether wxpython is the problem or my code, but the label always catches focus if I want to start my code. Even clicking somewhere else won't let go, and it is still focused. here is my Python code: import wx app = wx.App(False) frame = wx.Frame(None, wx.ID_ANY, ""Encrypto++"", size=(950, 600)) panel = wx.Panel(frame) Key_box = wx.TextCtrl(panel, pos=(40, 50), size=(500, 45)) frame.Show() app.MainLoop() I tried testing out more Labels but still had no success, chatgpt couldn't help me more.",[],0,"python, user-interface, wxpython, python, user-interface, wxpython",https://stackoverflow.com/questions/79156145/wxpython-label-catches-focus-on-start
51,Why is my extended kalman filtering implementation on satellite position not fitting my measurement data instead matching my prediction too much,"I implemented extended kalman filtering on python to filter noisy measurement data of the International Space Station's latitude and longitude projections on earth. However, the filter almost fits the mathematical model exactly and doesn't try to fit my noisy measurement data well at all. Despite changing values of P, Q and R my covariance and noise matrices, there is almost no difference until the filter starts behaving erratically. I modeled the math model in a fairly accurate way. It yields decent results as compared to NASA's TLE data. Therefore I don't think that is the issue. I believe the issue lies in my extended kalman filter implementation. I think the jacobian matrices I coded are also accurate. My code has 7 state vectors. Position in 3 coordinates, velocity in 3 coordinates and the semi major axis. In each loop of the filter I update those values and enter them in an observation_model that changes the values to latitude and longitude coordinates. The residual I get is in latitudes and longitudes and doesn't exceed the range of (-20, 20). But no matter how I change the P, Q and R values, I dont see a big difference in the final graphs I have attached to the question.The image is the plotted graph of longitude vs latitude in measurement, model and filter outputs My python code: import numpy as np import matplotlib.pyplot as plt from scipy.constants import G, pi from skyfield.api import load from datetime import timedelta #Constants mu = 3.986e14 # Gravitational parameter (m^3/s^2) Re = 6378100 # Earth radius in m Cd = 2.7 # Drag coefficient A = 1996.91 # Cross-sectional area in m^2 mass = 460460 # Mass of the ISS in kg earth_rotation_rate = 7.2921159e-5 # rad/s # Orbital Elements semi_major_axis = 6780000 # in m eccentricity = 0.001 inclination = np.radians(51.64) raan = np.radians(60) true_anomaly = np.radians(0) # starting point (initial anomaly) # Dynamic atmospheric density function based on altitude def atmospheric_density(altitude): # Approximate atmospheric density model (kg/m^3) altitude = altitude / 1000 return 1.02e07 * altitude**-7.172 # Calculate drag and update semi-major axis def calculate_drag(semi_major_axis, velocity, t): altitude = semi_major_axis - Re rho = atmospheric_density(401900) # print(rho) drag_acc = -0.5 * Cd * (A / mass) * rho * velocity**2 # Calculate the change in the semi-major axis using the drag formula delta_a_dot = - (2 * (semi_major_axis)**2 / mu) * drag_acc * velocity * t semi_major_axis_updated = semi_major_axis - delta_a_dot return semi_major_axis_updated def gravitational_acceleration(x, y, z): r = np.sqrt(x**2 + y**2 + z**2) g_x = -mu * x / r**3 g_y = -mu * y / r**3 g_z = -mu * z / r**3 return np.array([g_x, g_y, g_z]) def state_transition(state, delta_t): x, y, z, v_x, v_y, v_z, a = state # Define velocity and acceleration at current state def derivatives(s): x, y, z, v_x, v_y, v_z, _ = s accel_gravity = gravitational_acceleration(x, y, z) return np.array([ v_x, v_y, v_z, accel_gravity[0], accel_gravity[1], accel_gravity[2], 0 ]) # RK4 calculations k1 = derivatives(state) k2 = derivatives(state + 0.5 * delta_t * k1) k3 = derivatives(state + 0.5 * delta_t * k2) k4 = derivatives(state + delta_t * k3) # Update state new_state = state + (delta_t / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4) # Update semi-major axis with drag velocity = np.sqrt(new_state[3]**2 + new_state[4]**2 + new_state[5]**2) new_state[6] = calculate_drag(new_state[6], velocity, delta_t) r = np.sqrt(new_state[0]**2 + new_state[1]**2 + new_state[2]**2) return new_state def compute_F_jacobian(state, delta_t): x, y, z, v_x, v_y, v_z, a = state rho = atmospheric_density(401900) # Initialize the Jacobian F F = np.zeros((7, 7)) # Position derivatives F[0, 0] = 1 F[0, 3] = (-mu*delta_t*(-2*x**2 + y**2 + z**2))/(np.sqrt((x**2+y**2+z**2)**5)) F[0, 4] = (3*mu*delta_t*x*y)/(np.sqrt((x**2+y**2+z**2)**5)) F[0, 5] = (3*mu*delta_t*x*z)/(np.sqrt((x**2+y**2+z**2)**5)) F[1, 1] = 1 F[1, 3] = (3*mu*delta_t*x*y)/(np.sqrt((x**2+y**2+z**2)**5)) F[1, 4] = (-mu*delta_t*(-2*y**2 + x**2 + z**2))/(np.sqrt((x**2+y**2+z**2)**5)) F[1, 5] = (3*mu*delta_t*y*z)/(np.sqrt((x**2+y**2+z**2)**5)) F[2, 2] = 1 F[2, 3] = (3*mu*delta_t*x*z)/(np.sqrt((x**2+y**2+z**2)**5)) F[2, 4] = (3*mu*delta_t*y*z)/(np.sqrt((x**2+y**2+z**2)**5)) F[2, 5] = (-mu*delta_t*(-2*z**2 + x**2 + y**2))/(np.sqrt((x**2+y**2+z**2)**5)) # Velocity derivatives F[3, 0] = delta_t F[3, 3] = 1 - Cd*(A/mass)*rho*v_x*delta_t F[3, 6] = -3*(a**2/mu)*Cd*(A/mass)*rho*(v_x**2+v_y**2+v_z**2)**0.5*v_x*delta_t F[4, 1] = delta_t F[4, 4] = 1 - Cd*(A/mass)*rho*v_y*delta_t F[4, 6] = -3*(a**2/mu)*Cd*(A/mass)*rho*(v_x**2+v_y**2+v_z**2)**0.5*v_y*delta_t F[5, 2] = delta_t F[5, 5] = 1 - Cd*(A/mass)*rho*v_z*delta_t F[5, 6] = -3*(a**2/mu)*Cd*(A/mass)*rho*(v_x**2+v_y**2+v_z**2)**0.5*v_z*delta_t # Semi-major axis update derivative F[6, 6] = 1 - 2*(a/mu)*Cd*(A/mass)*rho*(v_x**2+v_y**2+v_z**2)**1.5*delta_t return F def observation_model(state, t): x, y, z = state[0], state[1], state[2] # Calculate radial distance from Earth's center # Latitude (in degrees) x_geo = (x * np.cos(raan) - y * np.sin(raan)) * np.cos(inclination) y_geo = x * np.sin(raan) + y * np.cos(raan) z_geo = y * np.sin(inclination) lat = np.arcsin(z_geo / np.sqrt(x_geo**2 + y_geo**2 + z_geo**2)) * 180 / np.pi lon = np.arctan2(y_geo, x_geo) * 180 / np.pi lon -= earth_rotation_rate * t * 180 / np.pi # convert rad to degrees return np.array([lat, lon]) def calculate_H_jacobian(state): x, y, z = state[0:3] H = np.zeros((2, 7)) H[0, 0] = -z * x / ((x**2 + y**2 + z**2) * np.sqrt(x**2 + y**2)) H[0, 1] = -z * y / ((x**2 + y**2 + z**2) * np.sqrt(x**2 + y**2)) H[0, 2] = np.sqrt(x**2 + y**2) / (x**2 + y**2 + z**2) H[1, 0] = -y / (x**2 + y**2) H[1, 1] = x / (x**2 + y**2) return H # Load TLE data for ISS from CelesTrak stations_url = 'https://celestrak.com/NORAD/elements/stations.txt' satellites = load.tle_file(stations_url) by_name = {sat.name: sat for sat in satellites} iss = by_name['ISS (ZARYA)'] # Define the timescale and custom time ts = load.timescale() t_custom = ts.utc(2024, 11, 9, 12, 54, 0) times = [] latitudes_meas = [] longitudes_meas = [] # Measure the ISS's position every minute for the next 90 minutes for i in range(90): t2 = t_custom + timedelta(minutes=i) geocentric = iss.at(t2) subpoint = geocentric.subpoint() latitude = subpoint.latitude.degrees longitude = subpoint.longitude.degrees # Store the data times.append(t2.utc_strftime('%Y-%m-%d %H:%M:%S')) latitudes_meas.append(latitude) longitudes_meas.append(longitude) noise_std = 2.5 # Noise level (standard deviation noisy_latitudes = latitudes_meas + np.random.normal(0, noise_std, len(latitudes_meas)) noisy_longitudes = longitudes_meas + np.random.normal(0, noise_std, len(longitudes_meas)) # Initialize a state vector with semi-major axis initial_state = np.array([ 5429.128332332650 * 1000, # Initial x (m) -975.311960132481 * 1000, # Initial y (m) 3957.346410376780 * 1000, # Initial z (m) -1.80166995818100 * 1000, # Initial x-velocity (m/s) 6.28473745611627 * 1000, # Initial y-velocity for circular orbit (m/s) 3.99918311510884 * 1000, # Initial z-velocity (m/s) 6780 * 1000 # Semi-major axis (m) ]) delta_t = 60 time_steps = 90 * 60 // delta_t results = np.zeros((time_steps, 7)) ttime = 0 ttimeobs = 0 observed_values = [] P = np.diag([2e3, 2e3, 2e3, 1e0, 1e0, 1e0, 2e0]) Q = np.diag([2e3, 3e3, 3e3, 2e0, 3e0, 2e0, 2e0]) R = np.diag([noise_std, noise_std]) current_state = initial_state for i in range(time_steps) : ttimeobs += i current_state = state_transition(current_state, delta_t) # Update state with refined delta_t # print(current_state) true_measurements = observation_model(current_state, ttimeobs) observed_values.append(true_measurements) lat_modeled = [] long_modeled = [] for i in observed_values: lat_modeled.append(i[0]) long_modeled.append(i[1]) for i in range(time_steps): ttime += i results[i] = current_state # print(results[i]) # Prediction Stage current_state = state_transition(current_state, delta_t) # Update state with refined delta_t F_jacobian = compute_F_jacobian(current_state, delta_t) P = F_jacobian @ P @ np.transpose(F_jacobian) + Q # print(P[0]) # Update Stage observation = observation_model(current_state, ttime) z = np.array([noisy_latitudes[i], noisy_longitudes[i]]) residual = z - observation print(i, residual, observation[1], z[1]) H_jacobian = calculate_H_jacobian(current_state) S = H_jacobian @ P @ np.transpose(H_jacobian) + R K = P @ np.transpose(H_jacobian) @ np.linalg.inv(S) # print(K) current_state = current_state + K @ residual # Updated state estimate P = (np.eye(len(current_state)) - K @ H_jacobian) @ P # Updated covariance estimate lat_filtered = [] long_filtered = [] ttime = 0 for i in range(len(results)): ttime += i observation = observation_model(results[i], ttime) lat_filtered.append(observation[0]) long_filtered.append(observation[1]) #Plotting longitude vs latitude plt.figure(figsize=(10, 5)) plt.plot(noisy_longitudes, noisy_latitudes, 'o-', color='blue', markersize=5) plt.plot(long_filtered, lat_filtered, 'o-', color='green', markersize=5) plt.plot(long_modeled, lat_modeled, 'o-', label='Mathematical Model (with perturbations)', color='red', markersize=5) plt.title('Longitude vs Latitude') plt.xlabel('Longitude (degrees)') plt.ylabel('Latitude (degrees)') plt.grid() plt.tight_layout() plt.show()",[],0,"python, kalman-filter, python, kalman-filter",https://stackoverflow.com/questions/79156139/why-is-my-extended-kalman-filtering-implementation-on-satellite-position-not-fit
51,Stochastic Weight Averaging (SWA) Gaussian Implementation,"This is part of homework and I have been tortured by it... The goal is to implement SWA-Gaussian proposed by Wesley J. Maddox on NeurIPS 2019. The algorithm is as follows: enter image description here I tried everything to implement it but always failed to pass the criterion on test dataset... I'd be appreciated if anyone could help me check my code and see if anything is wrong... I know this is long and annoying... If you still want to give a hand just look at code under #TODO mark... This will be my last try... from util import draw_reliability_diagram, cost_function, setup_seeds, calc_calibration_curve USE_PRETRAINED_MODEL = True """""" If `USE_PRETRAINED_MODEL` is `True`, then MAP inference uses provided pretrained weights. """""" class InferenceType(enum.Enum): """""" Inference mode switch for your implementation. `MAP` simply predicts the most likely class using pretrained MAP weights. `SWAG_DIAGONAL` and `SWAG_FULL` correspond to SWAG-diagonal and the full SWAG method, respectively. """""" MAP = 0 SWAG_DIAGONAL = 1 SWAG_FULL = 2 class SWAGInference(object): """""" Your implementation of SWA-Gaussian. This class is used to run and evaluate your solution. You must preserve all methods and signatures of this class. However, you can add new methods if you want. We provide basic functionality and some helper methods. You can pass all baselines by only modifying methods marked with TODO. However, we encourage you to skim other methods in order to gain a better understanding of SWAG. """""" def __init__( self, train_xs: torch.Tensor, model_dir: pathlib.Path, # TODO(1): change inference_mode to InferenceType.SWAG_DIAGONAL # TODO(2): change inference_mode to InferenceType.SWAG_FULL inference_mode: InferenceType = InferenceType.SWAG_FULL, # TODO(2): optionally add/tweak hyperparameters swag_training_epochs: int = 24, swag_lr: float = 0.045, # original: 0.045 swag_update_interval: int = 1, max_rank_deviation_matrix: int = 15, num_bma_samples: int = 30, ): """""" :param train_xs: Training images (for storage only) :param model_dir: Path to directory containing pretrained MAP weights :param inference_mode: Control which inference mode (MAP, SWAG-diagonal, full SWAG) to use :param swag_training_epochs: Total number of gradient descent epochs for SWAG :param swag_lr: Learning rate for SWAG gradient descent :param swag_update_interval: Frequency (in epochs) for updating SWAG statistics during gradient descent :param max_rank_deviation_matrix: Rank of deviation matrix for full SWAG :param num_bma_samples: Number of networks to sample for Bayesian model averaging during prediction """""" self.model_dir = model_dir self.inference_mode = inference_mode self.swag_training_epochs = swag_training_epochs self.swag_lr = swag_lr self.swag_update_interval = swag_update_interval self.max_rank_deviation_matrix = max_rank_deviation_matrix self.num_bma_samples = num_bma_samples # Network used to perform SWAG. # Note that all operations in this class modify this network IN-PLACE! self.network = CNN(in_channels=3, out_classes=6) # Store training dataset to recalculate batch normalization statistics during SWAG inference self.training_dataset = torch.utils.data.TensorDataset(train_xs) # SWAG-diagonal # TODO(1): create attributes for SWAG-diagonal # Hint: self._create_weight_copy() creates an all-zero copy of the weights # as a dictionary that maps from weight name to values. # Hint: you never need to consider the full vector of weights, # but can always act on per-layer weights (in the format that _create_weight_copy() returns) self.iteration_num = 0 self.parameter_swa = self._create_weight_copy() self.parameter_swa_2ndmoment = self._create_weight_copy() # END(1) # Full SWAG # TODO(2): create attributes for SWAG-full # Hint: check collections.deque if self.inference_mode == InferenceType.SWAG_FULL: self.parameter_D = collections.deque() for _ in range(self.max_rank_deviation_matrix): self.parameter_D.append(self._create_weight_copy()) # would contain weight_copy for self.max_rank_deviation_matrix times # END(2) # Calibration, prediction, and other attributes # TODO(2): create additional attributes, e.g., for calibration self._calibration_threshold = None # this is an example, feel free to be creative def update_swag_statistics(self) -> None: """""" Update SWAG statistics with the current weights of self.network. """""" self.iteration_num = self.iteration_num + 1 if self.iteration_num % self.swag_update_interval != 0: return # Create a copy of the current network weights copied_params = {name: param.detach() for name, param in self.network.named_parameters()} # Full SWAG if self.inference_mode == InferenceType.SWAG_FULL: # TODO(2): update full SWAG attributes for weight `name` using `copied_params` and `param` self.parameter_D.popleft() self.parameter_D.append({}) # raise NotImplementedError(""Update full SWAG statistics"") # END(2) # SWAG-diagonal for name, param in copied_params.items(): # TODO(1): update SWAG-diagonal attributes for weight `name` using `copied_params` and `param` update_num = self.iteration_num // self.swag_update_interval for name, param in self.network.named_parameters(): self.parameter_swa[name] = (update_num * self.parameter_swa[name] + param) / (update_num + 1) self.parameter_swa_2ndmoment[name] = (update_num * self.parameter_swa_2ndmoment[name] + param**2) / (update_num + 1) if self.inference_mode == InferenceType.SWAG_FULL: self.parameter_D[-1][name] = param - self.parameter_swa[name] # raise NotImplementedError(""Update SWAG-diagonal statistics"") # END(1) def fit_swag_model(self, loader: torch.utils.data.DataLoader) -> None: """""" Fit SWAG on top of the pretrained network self.network. This method should perform gradient descent with occasional SWAG updates by calling self.update_swag_statistics(). """""" # We use SGD with momentum and weight decay to perform SWA. # See the paper on how weight decay corresponds to a type of prior. # Feel free to play around with optimization hyperparameters. optimizer = torch.optim.SGD( self.network.parameters(), lr=self.initial_lr, momentum=0.9, nesterov=False, weight_decay=1e-4, # original: 1e-4 ) loss_fn = torch.nn.CrossEntropyLoss( reduction=""mean"", ) # TODO(2): Update SWAGScheduler instantiation if you decided to implement a custom schedule. # By default, this scheduler just keeps the initial learning rate given to `optimizer`. lr_scheduler = SWAGScheduler( optimizer, epochs=self.swag_training_epochs, steps_per_epoch=len(loader), initial_lr=self.initial_lr, final_lr=self.final_lr ) # TODO(1): Perform initialization for SWAG fitting self.parameter_swa = {name: param.detach() for name, param in self.network.named_parameters()} # Init with pretrained parameters self.parameter_swa_2ndmoment = {name: param.detach()**2 for name, param in self.network.named_parameters()} # Init with pretrained parameters # raise NotImplementedError(""Initialize SWAG fitting"") # END(1) self.network.train() with tqdm.trange(self.swag_training_epochs, desc=""Running gradient descent for SWA"") as pbar: progress_dict = {} for epoch in pbar: avg_loss = 0.0 avg_accuracy = 0.0 num_samples = 0 for batch_images, batch_snow_labels, batch_cloud_labels, batch_labels in loader: optimizer.zero_grad() predictions = self.network(batch_images) batch_loss = loss_fn(input=predictions, target=batch_labels) batch_loss.backward() optimizer.step() progress_dict[""lr""] = lr_scheduler.get_last_lr()[0] lr_scheduler.step() # Calculate cumulative average training loss and accuracy avg_loss = (batch_images.size(0) * batch_loss.item() + num_samples * avg_loss) / ( num_samples + batch_images.size(0) ) avg_accuracy = ( torch.sum(predictions.argmax(dim=-1) == batch_labels).item() + num_samples * avg_accuracy ) / (num_samples + batch_images.size(0)) num_samples += batch_images.size(0) progress_dict[""avg. epoch loss""] = avg_loss progress_dict[""avg. epoch accuracy""] = avg_accuracy pbar.set_postfix(progress_dict) # TODO(1): Implement periodic SWAG updates using the attributes defined in __init__ # raise NotImplementedError(""Periodically update SWAG statistics"") self.update_swag_statistics() # END(1) def apply_calibration(self, validation_data: torch.utils.data.Dataset) -> None: """""" Calibrate your predictions using a small validation set. validation_data contains well-defined and ambiguous samples, where you can identify the latter by having label -1. """""" if self.inference_mode == InferenceType.MAP: # In MAP mode, simply predict argmax and do nothing else self._calibration_threshold = 0.0 return # TODO(1): pick a prediction threshold, either constant or adaptive. # The provided value should suffice to pass the easy baseline. self._calibration_threshold = 2.0 / 3.0 # TODO(2): perform additional calibration if desired. # Feel free to remove or change the prediction threshold. val_images, val_snow_labels, val_cloud_labels, val_labels = validation_data.tensors assert val_images.size() == (140, 3, 60, 60) # N x C x H x W assert val_labels.size() == (140,) assert val_snow_labels.size() == (140,) assert val_cloud_labels.size() == (140,) def predict_probabilities_swag(self, loader: torch.utils.data.DataLoader) -> torch.Tensor: """""" Perform Bayesian model averaging using your SWAG statistics and predict probabilities for all samples in the loader. Outputs should be a Nx6 tensor, where N is the number of samples in loader, and all rows of the output should sum to 1. That is, output row i column j should be your predicted p(y=j | x_i). """""" self.network.eval() # Perform Bayesian model averaging: # Instead of sampling self.num_bma_samples networks (using self.sample_parameters()) # for each datapoint, you can save time by sampling self.num_bma_samples networks, # and perform inference with each network on all samples in loader. model_predictions = [] for _ in tqdm.trange(self.num_bma_samples, desc=""Performing Bayesian model averaging""): # TODO(1): Sample new parameters for self.network from the SWAG approximate posterior # raise NotImplementedError(""Sample network parameters"") self.sample_parameters() # END(1) # TODO(1): Perform inference for all samples in `loader` using current model sample, # and add the predictions to model_predictions # raise NotImplementedError(""Perform inference using current model"") model_predictions.append(self.predict_probabilities_map(loader)) # END(1) assert len(model_predictions) == self.num_bma_samples assert all( isinstance(sample_predictions, torch.Tensor) and sample_predictions.dim() == 2 # N x C and sample_predictions.size(1) == 6 for sample_predictions in model_predictions ) # TODO(1): Average predictions from different model samples into bma_probabilities # raise NotImplementedError(""Aggregate predictions from model samples"") bma_probabilities = torch.stack(model_predictions, dim=0).mean(dim=0) # END(1) assert bma_probabilities.dim() == 2 and bma_probabilities.size(1) == 6 # N x C return bma_probabilities def sample_parameters(self) -> None: """""" Sample a new network from the approximate SWAG posterior. For simplicity, this method directly modifies self.network in-place. Hence, after calling this method, self.network corresponds to a new posterior sample. """""" # Instead of acting on a full vector of parameters, all operations can be done on per-layer parameters. for name, param in self.network.named_parameters(): # SWAG-diagonal part z_diag = torch.randn(param.size()) # TODO(1): Sample parameter values for SWAG-diagonal # raise NotImplementedError(""Sample parameter for SWAG-diagonal"") mean_weights = self.parameter_swa[name] std_weights = torch.clamp(self.parameter_swa_2ndmoment[name] - self.parameter_swa[name] ** 2, min=0) ** 0.5 # END(1) assert mean_weights.size() == param.size() and std_weights.size() == param.size() # Diagonal part sampled_weight = mean_weights + std_weights * z_diag # Full SWAG part if self.inference_mode == InferenceType.SWAG_FULL: # TODO(2): Sample parameter values for full SWAG # raise NotImplementedError(""Sample parameter for full SWAG"") matrix_D = torch.stack([item[name] for item in self.parameter_D], dim=-1) z_cov = torch.randn(self.max_rank_deviation_matrix) sampled_weight = mean_weights + 0.5**0.5 * std_weights * z_diag + 0.5**0.5 * matrix_D @ z_cov / (self.max_rank_deviation_matrix - 1)**0.5 # END(2) # Modify weight value in-place; directly changing self.network param.data = sampled_weight # TODO(1): Don't forget to update batch normalization statistics using self._update_batchnorm_statistics() # in the appropriate place! # raise NotImplementedError(""Update batch normalization statistics for newly sampled network"") self._update_batchnorm_statistics() # END(1) def predict_labels(self, predicted_probabilities: torch.Tensor) -> torch.Tensor: """""" Predict labels in {0, 1, 2, 3, 4, 5} or ""don't know"" as -1 based on your model's predicted probabilities. The parameter predicted_probabilities is an Nx6 tensor containing predicted probabilities as returned by predict_probabilities(...). The output should be a N-dimensional long tensor, containing values in {-1, 0, 1, 2, 3, 4, 5}. """""" # label_probabilities contains the per-row maximum values in predicted_probabilities, # max_likelihood_labels the corresponding column index (equivalent to class). label_probabilities, max_likelihood_labels = torch.max(predicted_probabilities, dim=-1) num_samples, num_classes = predicted_probabilities.size() assert label_probabilities.size() == (num_samples,) and max_likelihood_labels.size() == (num_samples,) # A model without uncertainty awareness might simply predict the most likely label per sample: # return max_likelihood_labels # A bit better: use a threshold to decide whether to return a label or ""don't know"" (label -1) # TODO(2): implement a different decision rule if desired return torch.where( label_probabilities >= self._calibration_threshold, max_likelihood_labels, torch.ones_like(max_likelihood_labels) * -1, ) def _create_weight_copy(self) -> typing.Dict[str, torch.Tensor]: """"""Create an all-zero copy of the network weights as a dictionary that maps name -> weight"""""" return { name: torch.zeros_like(param, requires_grad=False) for name, param in self.network.named_parameters() } def fit( self, loader: torch.utils.data.DataLoader, ) -> None: """""" Perform full SWAG fitting procedure. If `PRETRAINED_WEIGHTS_FILE` is `True`, this method skips the MAP inference part, and uses pretrained weights instead. """""" # MAP inference to obtain initial weights PRETRAINED_WEIGHTS_FILE = self.model_dir / ""map_weights.pt"" if USE_PRETRAINED_MODEL: self.network.load_state_dict(torch.load(PRETRAINED_WEIGHTS_FILE)) print(""Loaded pretrained MAP weights from"", PRETRAINED_WEIGHTS_FILE) else: self.fit_map_model(loader) # SWAG if self.inference_mode in (InferenceType.SWAG_DIAGONAL, InferenceType.SWAG_FULL): self.fit_swag_model(loader) def fit_map_model(self, loader: torch.utils.data.DataLoader) -> None: """""" MAP inference procedure to obtain initial weights of self.network. This is the exact procedure that was used to obtain the pretrained weights we provide. """""" map_training_epochs = 140 initial_learning_rate = 0.01 reduced_learning_rate = 0.0001 start_decay_epoch = 50 decay_factor = reduced_learning_rate / initial_learning_rate # Create optimizer, loss, and a learning rate scheduler that aids convergence optimizer = torch.optim.SGD( self.network.parameters(), lr=initial_learning_rate, momentum=0.9, nesterov=False, weight_decay=1e-4, ) loss_fn = torch.nn.CrossEntropyLoss( reduction=""mean"", ) lr_scheduler = torch.optim.lr_scheduler.SequentialLR( optimizer, [ torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0), torch.optim.lr_scheduler.LinearLR( optimizer, start_factor=1.0, end_factor=decay_factor, total_iters=(map_training_epochs - start_decay_epoch) * len(loader), ), ], milestones=[start_decay_epoch * len(loader)], ) # Put network into training mode # Batch normalization layers are only updated if the network is in training mode, # and are replaced by a moving average if the network is in evaluation mode. self.network.train() with tqdm.trange(map_training_epochs, desc=""Fitting initial MAP weights"") as pbar: progress_dict = {} # Perform the specified number of MAP epochs for epoch in pbar: avg_loss = 0.0 avg_accuracy = 0.0 num_samples = 0 # Iterate over batches of randomly shuffled training data for batch_images, _, _, batch_labels in loader: # Training step optimizer.zero_grad() predictions = self.network(batch_images) batch_loss = loss_fn(input=predictions, target=batch_labels) batch_loss.backward() optimizer.step() # Save learning rate that was used for step, and calculate new one progress_dict[""lr""] = lr_scheduler.get_last_lr()[0] with warnings.catch_warnings(): # Suppress annoying warning (that we cannot control) inside PyTorch warnings.simplefilter(""ignore"") lr_scheduler.step() # Calculate cumulative average training loss and accuracy avg_loss = (batch_images.size(0) * batch_loss.item() + num_samples * avg_loss) / ( num_samples + batch_images.size(0) ) avg_accuracy = ( torch.sum(predictions.argmax(dim=-1) == batch_labels).item() + num_samples * avg_accuracy ) / (num_samples + batch_images.size(0)) num_samples += batch_images.size(0) progress_dict[""avg. epoch loss""] = avg_loss progress_dict[""avg. epoch accuracy""] = avg_accuracy pbar.set_postfix(progress_dict) def predict_probabilities(self, xs: torch.Tensor) -> torch.Tensor: """""" Predict class probabilities for the given images xs. This method returns an NxC float tensor, where row i column j corresponds to the probability that y_i is class j. This method uses different strategies depending on self.inference_mode. """""" self.network = self.network.eval() # Create a loader that we can deterministically iterate many times if necessary loader = torch.utils.data.DataLoader( torch.utils.data.TensorDataset(xs), batch_size=32, shuffle=False, num_workers=0, drop_last=False, ) with torch.no_grad(): # save memory by not tracking gradients if self.inference_mode == InferenceType.MAP: return self.predict_probabilities_map(loader) else: return self.predict_probabilities_swag(loader) def predict_probabilities_map(self, loader: torch.utils.data.DataLoader) -> torch.Tensor: """""" Predict probabilities assuming that self.network is a MAP estimate. """""" all_predictions = [] for (batch_images,) in loader: all_predictions.append(self.network(batch_images)) all_predictions = torch.cat(all_predictions) return torch.softmax(all_predictions, dim=-1) def _update_batchnorm_statistics(self) -> None: """""" Reset and fit batch normalization statistics using the training dataset self.training_dataset. Batch normalization usually uses an exponential moving average, controlled by the `momentum` parameter. However, we are not training but want the statistics for the full training dataset. Hence, setting `momentum` to `None` tracks a cumulative average instead. The following code stores original `momentum` values, sets all to `None`, and restores the previous hyperparameters after updating batchnorm statistics. """""" original_momentum_values = dict() for module in self.network.modules(): # Only need to handle batchnorm modules if not isinstance(module, torch.nn.modules.batchnorm._BatchNorm): continue # Store old momentum value before removing it original_momentum_values[module] = module.momentum module.momentum = None # Reset batch normalization statistics module.reset_running_stats() loader = torch.utils.data.DataLoader( self.training_dataset, batch_size=32, shuffle=False, num_workers=0, drop_last=False, ) self.network.train() for (batch_images,) in loader: self.network(batch_images) self.network.eval() # Restore old `momentum` hyperparameter values for module, momentum in original_momentum_values.items(): module.momentum = momentum def evaluate( swag_inference: SWAGInference, eval_dataset: torch.utils.data.Dataset, extended_evaluation: bool, output_location: pathlib.Path, ) -> None: print(""Evaluating model on validation data"") # We ignore is_snow and is_cloud here, but feel free to use them as well images, snow_labels, cloud_labels, labels = eval_dataset.tensors # Predict class probabilities on test data, # most likely classes (according to the max predicted probability), # and classes as predicted by your SWAG implementation. all_pred_probabilities = swag_inference.predict_probabilities(images) max_pred_probabilities, argmax_pred_labels = torch.max(all_pred_probabilities, dim=-1) predicted_labels = swag_inference.predict_labels(all_pred_probabilities) # Create a mask that ignores ambiguous samples (those with class -1) non_ambiguous_mask = labels != -1 # Calculate three kinds of accuracy: # 1. Overall accuracy, counting ""don't know"" (-1) as its own class # 2. Accuracy on all samples that have a known label. Predicting -1 on those counts as wrong here. # 3. Accuracy on all samples that have a known label w.r.t. the class with the highest predicted probability. overall_accuracy = torch.mean((predicted_labels == labels).float()).item() non_ambiguous_accuracy = torch.mean((predicted_labels[non_ambiguous_mask] == labels[non_ambiguous_mask]).float()).item() non_ambiguous_argmax_accuracy = torch.mean( (argmax_pred_labels[non_ambiguous_mask] == labels[non_ambiguous_mask]).float() ).item() print(f""Accuracy (raw): {overall_accuracy:.4f}"") print(f""Accuracy (non-ambiguous only, your predictions): {non_ambiguous_accuracy:.4f}"") print(f""Accuracy (non-ambiguous only, predicting most-likely class): {non_ambiguous_argmax_accuracy:.4f}"") # Determine which threshold would yield the smallest cost on the validation data # Note that this threshold does not necessarily generalize to the test set! # However, it can help you judge your method's calibration. threshold_values = [0.0] + list(torch.unique(max_pred_probabilities, sorted=True)) costs = [] for threshold in threshold_values: thresholded_predictions = torch.where(max_pred_probabilities <= threshold, -1 * torch.ones_like(predicted_labels), predicted_labels) costs.append(cost_function(thresholded_predictions, labels).item()) best_threshold_index = np.argmin(costs) print(f""Best cost {costs[best_threshold_index]} at threshold {threshold_values[best_threshold_index]}"") print(""Note that this threshold does not necessarily generalize to the test set!"") # Calculate ECE and plot the calibration curve calibration_data = calc_calibration_curve(all_pred_probabilities.numpy(), labels.numpy(), num_bins=20) print(""Validation ECE:"", calibration_data[""ece""]) class CNN(torch.nn.Module): def __init__( self, in_channels: int, out_classes: int, ): super().__init__() self.layer0 = torch.nn.Sequential( torch.nn.Conv2d(in_channels, 32, kernel_size=5), torch.nn.BatchNorm2d(32), torch.nn.ReLU(), ) self.layer1 = torch.nn.Sequential( torch.nn.Conv2d(32, 32, kernel_size=3), torch.nn.BatchNorm2d(32), torch.nn.ReLU(), ) self.layer2 = torch.nn.Sequential( torch.nn.Conv2d(32, 32, kernel_size=3), torch.nn.BatchNorm2d(32), torch.nn.ReLU(), ) self.pool1 = torch.nn.MaxPool2d((2, 2), stride=(2, 2)) self.layer3 = torch.nn.Sequential( torch.nn.Conv2d(32, 64, kernel_size=3), torch.nn.BatchNorm2d(64), torch.nn.ReLU(), ) self.layer4 = torch.nn.Sequential( torch.nn.Conv2d(64, 64, kernel_size=3), torch.nn.BatchNorm2d(64), torch.nn.ReLU(), ) self.pool2 = torch.nn.MaxPool2d((2, 2), stride=(2, 2)) self.layer5 = torch.nn.Sequential( torch.nn.Conv2d(64, 64, kernel_size=3), ) self.global_pool = torch.nn.AdaptiveAvgPool2d((1, 1)) self.linear = torch.nn.Linear(64, out_classes) def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.layer0(x) x = self.layer1(x) x = self.layer2(x) x = self.pool1(x) x = self.layer3(x) x = self.layer4(x) x = self.pool2(x) x = self.layer5(x) # Average features over both spatial dimensions, and remove the now superfluous dimensions x = self.global_pool(x).squeeze(-1).squeeze(-1) log_softmax = self.linear(x) return log_softmax",[],0,"python, sampling, bayesian-deep-learning, python, sampling, bayesian-deep-learning",https://stackoverflow.com/questions/79156127/stochastic-weight-averaging-swa-gaussian-implementation
51,How to Add Custom Report Button to Sale Order in Odoo?,"I'm currently working on a custom Odoo module in Odoo where I want to add a custom report button called ""Order Product Summary"" to the Sale Order form. The goal is to create a summary report that aggregates the quantities of products across selected orders. I've created a custom module with the following folder structure: My Code is as Follows: __manifest__.py { 'name': 'Order Product Summary Report', 'version': '1.0', 'depends': ['sale'], 'data': [ 'report/report_product_summary.xml', ], 'installable': True, 'application': False, } sale_order_summary.py from odoo import models, fields class SaleOrder(models.Model): _inherit = 'sale.order' product_summary_count = fields.Integer(string=""Product Summary Count"", compute=""_compute_product_summary"") def _compute_product_summary(self): for order in self: order.product_summary_count = sum(line.product_uom_qty for line in order.order_line) report_product_summary.xml <?xml version=""1.0"" encoding=""UTF-8""?> <odoo> <!-- Report Template --> <template id=""report_product_summary_template""> <t t-call=""web.external_layout""> <div class=""page""> <h2>Order Product Summary Report</h2> <table class=""table table-sm table-hover""> <thead> <tr> <th>Product</th> <th>Quantity</th> </tr> </thead> <tbody> <t t-foreach=""doc.order_line"" t-as=""line""> <tr> <td><span t-field=""line.product_id.name""/></td> <td><span t-field=""line.product_uom_qty"" /></td> </tr> </t> </tbody> </table> </div> </t> </template> <!-- Report Action --> <report id=""action_report_product_summary"" string=""Order Product Summary"" model=""sale.order"" report_type=""qweb-pdf"" name=""order_product_summary_report.report_product_summary_template"" file=""order_product_summary_report.report_product_summary_template"" print_report_name=""'Order Product Summary - %s' % (object.name)"" /> </odoo> Despite following the steps to create and register the report button, it does not show up in the Odoo interface. I have checked for syntax errors in my XML files, ensured that the user has the necessary permissions, and looked at the server logs for any warnings or errors during loading. I am also unsure if I need to define anything else in my report XML to ensure it is properly registered. What additional steps should I take to ensure that my custom report button appears in the Sale Order form in Odoo? Are there any common pitfalls or mistakes I should look out for? Any guidance or advice would be greatly appreciated! Thank you!",[],0,"python, odoo, odoo-8, pyxml, python, odoo, odoo-8, pyxml",https://stackoverflow.com/questions/79156114/how-to-add-custom-report-button-to-sale-order-in-odoo
51,CDF of the sum of two random variables not giving the expected result,"I'm trying to numerically calculate the CDF of the sum of two random variables and I'm having trouble replicating answers on StackOverflow. This question CDF of a sum of independent random variables and this question Cumulative Distribution Function of 𝑋+𝑌 , where 𝑋,𝑌 are independent is convolution of 𝐹𝑋 and 𝐹𝑌 ? suggest this relationship is: 𝐹𝑋+𝑌(𝑎)=(𝐹𝑋∗𝑔)(a) I've numerically calculated a CDF and a PDF for two independent random variables. Here's the PDF and the CDF: PDF and CDF but when I convolve them using: np.convolve(a=pdf, v=cdf) I get something that plainly isn't a CDF: Not a CDF On thinking about this, I can't see how the convolution of a CDF and PDF can give a CDF, surely the convolution will end up with something that breaks the rules for CDFs (non-decreasing, goes to 1 etc.). I can't see how digitizing the continuous functions is responsible for my problems either. Can anyone state how I might get the CDF for X + Y from the PDF and CDF of X and Y if I have the digitized data? Are there numerical (digitization) issues I'm missing?","['[[ANSWER_1], [VOTES=0]] : What is going wrong? You convolved the PDF of one variable with the CDF of the other. np.convolve(a=pdf, v=cdf) Problem: Convolving a PDF with a CDF does not yield a valid PDF or CDF. The convolution operation for summing random variables specifically requires convolving the PDFs of the variables. Incorrect Formula: 𝐹𝑋+𝑌(𝑎)=(𝐹𝑋∗𝑔)(a) How to Correct It: Convolve the PDFs of X and Y to get the PDF of Z= X +Y and then Integrate the resulting PDF to obtain the CDF of Z.']",0,"python, statistics, probability, cdf, python, statistics, probability, cdf",https://stackoverflow.com/questions/79156072/cdf-of-the-sum-of-two-random-variables-not-giving-the-expected-result
51,Concatenate strings from several rows using Pandas groupby,"I want to apply some sort of concatenation of the strings in a column using groupby. This is my code so far: import pandas as pd from io import StringIO data = StringIO("""""" ""name1"",""hej"",""2014-11-01"" ""name1"",""du"",""2014-11-02"" ""name1"",""aj"",""2014-12-01"" ""name1"",""oj"",""2014-12-02"" ""name2"",""fin"",""2014-11-01"" ""name2"",""katt"",""2014-11-02"" ""name2"",""mycket"",""2014-12-01"" ""name2"",""lite"",""2014-12-01"" """""") # load string as stream into dataframe df = pd.read_csv(data,header=0, names=[""name"",""text"",""date""],parse_dates=[2]) # add column with month df[""month""] = df[""date""].apply(lambda x: x.month) I want the end result to look like this:","[""[[ANSWER_1], [VOTES=149]] : We can groupby the 'name' and 'month' columns, then call agg() functions of Panda’s DataFrame objects. The aggregation functionality provided by the agg() function allows multiple statistics to be calculated per group in one calculation. df.groupby(['name', 'month'], as_index = False).agg({'text': ' '.join})"", ""[[ANSWER_2], [VOTES=68]] : The answer by EdChum provides you with a lot of flexibility but if you just want to concateate strings into a column of list objects you can also: output_series = df.groupby(['name','month'])['text'].apply(list)"", '[[ANSWER_3], [VOTES=19]] : If you want to concatenate your ""text"" in a list: df.groupby([\'name\', \'month\'], as_index = False).agg({\'text\': list})', ""[[ANSWER_4], [VOTES=13]] : For me the above solutions were close but added some unwanted /n's and dtype:object, so here's a modified version: df.groupby(['name', 'month'])['text'].apply(lambda text: ''.join(text.to_string(index=False))).str.replace('(\\\\n)', '').reset_index()"", ""[[ANSWER_5], [VOTES=6]] : Please try this line of code : - df.groupby(['name','month'])['text'].apply(','.join).reset_index()"", ""[[ANSWER_6], [VOTES=3]] : Although, this is an old question. But just in case. I used the below code and it seems to work like a charm. text = ''.join(df[df['date'].dt.month==8]['text'])"", '[[ANSWER_7], [VOTES=3]] : Thanks to all the other answers, the following is probably the most concise and feels more natural. Using df.groupby(""X"")[""A""].agg() aggregates over one or many selected columns. df = pandas.DataFrame({\'A\' : [\'a\', \'a\', \'b\', \'c\', \'c\'], \'B\' : [\'i\', \'j\', \'k\', \'i\', \'j\'], \'X\' : [1, 2, 2, 1, 3]}) A B X a i 1 a j 2 b k 2 c i 1 c j 3 df.groupby(""X"", as_index=False)[""A""].agg(\' \'.join) X A 1 a c 2 a b 3 c df.groupby(""X"", as_index=False)[[""A"", ""B""]].agg(\' \'.join) X A B 1 a c i i 2 a b j k 3 c j']",249,"python, python-3.x, pandas, group-by, python, python-3.x, pandas, group-by",https://stackoverflow.com/questions/79155985/merge-rows-based-on-first-column-and-concatenate-all-other-columns-with-comma-de
51,Understanding Python Polars Errors,"I ran a Python Polars code and I got this error. Unfortunately, I cannot share my code, as a result, I just want to know what the error mean, as I can't seem to find out what it means on google. Below is the error: 'pyo3_runtime.PanicException: called Result::unwrap() on an Err value: OutOfSpec(""validity mask length must match the number of values"")' I am trying to run my python code that uses Polars, a better understanding of what the error mean will go a long way in helping me solve the issue. Thank you","[""[[ANSWER_1], [VOTES=-1]] : Means there's a mismatch in the length of the validity mask (used for null values) and the number of data values in a column. Check if the data you're working with has consistent lengths for columns and their null masks.""]",-2,"python, python-polars, python, python-polars",https://stackoverflow.com/questions/79155947/understanding-python-polars-errors
51,Using IRC how can I get a list of current viewers of my twitch stream?,"Using twitches IRC API I am trying to get all the current viewers of my stream when the bot runs however on_names never runs even though when the bot connects it should receive JOIN, 353 and 366 messages but only on_join is executed. (pubmsg, welcome, and join all work) import irc.client def on_connect(connection, event): logging.info(f""Connected to {connection.server}"") connection.join(channel) def on_join(connection, event): logging.info(f""Joined {channel}"") logging.info(f""event: {event}"") def on_names(connection, event): logging.info(f""Received raw message: {event} - Type: {event.type}"") # Check for NAMES response if event.type == ""353"": # NAMES reply # Format: :tmi.twitch.tv 353 <bot_nick> = <channel> :<names> channel = event.arguments[1] names = event.arguments[2].split() global viewers viewers = names logging.info(f""Updated list of viewers in {channel}: {viewers}"") elif event.type == ""366"": # End of NAMES reply logging.info(f""End of NAMES list for {event.arguments[1]}."") def run_irc_bot(server, port, nickname, token): reactor = irc.client.Reactor() factory = irc.connection.Factory(wrapper=ssl.wrap_socket) try: logging.info(""Connecting to chat..."") connection = reactor.server().connect(server, port, nickname, token, connect_factory=factory) logging.info(f""Connected to {server} as {nickname}"") except irc.client.ServerConnectionError as e: logging.error(f""Could not connect to server: {e}"") return connection.add_global_handler('welcome', on_connect) connection.add_global_handler('join', on_join) connection.add_global_handler('pubmsg', on_pubmsg) connection.add_global_handler(353, on_names) connection.add_global_handler(366, on_names) try: reactor.process_forever() except KeyboardInterrupt: logging.info(""Shutting down..."") finally: connection.close() logging.info(""Bot disconnected."") I have also tried connection.add_global_handler('names', on_names) connection.add_global_handler('353', on_names) connection.add_global_handler('366', on_names)",[],0,"python, irc, twitch-api, python, irc, twitch-api",https://stackoverflow.com/questions/79155930/using-irc-how-can-i-get-a-list-of-current-viewers-of-my-twitch-stream
51,how to keep window color changed in pygame,"Im trying to make it so when my picture bounces, the window changes to a random color, but when it changes, it only changes for a split second when it collides then the window changes back to white when i remove screen.fill, it keeps the random color for every bounce, but then it leaves a trail of images import pygame import random height = 500 width = 500 pygame.init() window = pygame.display.set_mode((500, 500)) pygame.display.set_caption('epic trampoline') colors = ['red', 'orange', 'yellow', 'green', 'blue', 'purple', 'pink'] running = True win_color = 'white' trampoline_color = 'black' clock = pygame.time.Clock() image = pygame.image.load('epicface.png').convert_alpha() epic_image = pygame.transform.scale(image, (50, 50)) # changing size epic_rect = epic_image.get_rect(center=(250, 0)) gravity = 0 acceleration = 1 while running: for event in pygame.event.get(): if event.type == pygame.QUIT: running = False clock.tick(30) gravity += 0.6 window.fill('white') epic_rect.bottom += gravity # rect = pygame.draw.rect(window, 'white', epic_rect) trampoline = pygame.draw.rect(window, trampoline_color, pygame.Rect(0, 440, 500, 10)) collide = pygame.Rect.colliderect(trampoline, epic_rect) if collide: epic_rect.bottom = trampoline.top win_color = window.fill(random.choice(colors)) gravity = -18 window.blit(epic_image, epic_rect.topleft) pygame.display.flip() pygame.display.flip()","[""[[ANSWER_1], [VOTES=0]] : Create a variable for the colur and use it to fill the window. Change the variable when there is a collision fill_color = 'white' trampoline = pygame.Rect(0, 440, 500, 10) while running: for event in pygame.event.get(): if event.type == pygame.QUIT: running = False clock.tick(30) gravity += 0.6 epic_rect.bottom += gravity collide = pygame.Rect.colliderect(trampoline, epic_rect) if collide: epic_rect.bottom = trampoline.top fill_color = random.choice(colors) gravity = -18 window.fill(fill_color) pygame.draw.rect(window, trampoline_color, trampoline) window.blit(epic_image, epic_rect.topleft) pygame.display.flip()""]",0,"python, image, pygame, collision-detection, rect, python, image, pygame, collision-detection, rect",https://stackoverflow.com/questions/79155928/how-to-keep-window-color-changed-in-pygame
51,Wagtail CMS - Programmatically enabling user access to manage Pages within the Admin panel,"Context Wagtail CMS has a permission system that builds on that of Django's. However, customizing it for users that are neither an admin nor using the pre-made groups Moderator or Editor is unclear. Presently, I have: A custom user class, StudentUser Pages arranged in the below hierarchy: Program | Course / | \ Report Labs Events I'd like to add a group, Student, which can add/submit pages of type Report. Like stated in the title, I require a programmatic solution. Having an admin go through and personally assign permissions is not acceptable. Problem Wagtail provides only one programmatic code example, which is for adding a custom permission here in their documentation: from django.contrib.auth.models import Permission from django.contrib.contenttypes.models import ContentType from wagtail.admin.models import Admin content_type = ContentType.objects.get_for_model(Admin) permission = Permission.objects.create( content_type=content_type, codename=""can_do_something"", name=""Can do something"", ) Technically, I don't need a custom permission. I need to grant a Group a custom set of existing permissions. To accomplish this, I've attempted the following: from django.contrib.auth.models import Permission, Group from example.website.models import StudentUser group, created = Group.objects.get_or_create(name=""Student"") add_report = Permission.objects.get(codename=""add_reportpage"") change_report = Permission.objects.get(codename=""change_reportpage"") group.permissions.add(add_report, change_report) user = StudentUser.objects.get(email=""[email protected]"") user.groups.add(group) Unfortunately, this hasn't worked. When I log into the backend with a user of type StudentUser (the account to which I provided the permission), there's nothing but the Documents tab (that is default) in the navigation menu. Nowhere can I see a place to modify Report. If I try to copy the exact Report URL path used in the admin login, it doesn't allow access when logged in the StudentUser despite the added permissions. Further debugging To figure out if there are some other types of permissions I'm missing, I listed the permissions for all groups. Then, I copied them for my new group. You can see the list of permissions now below that I copied from the built-in Moderators group: Moderators <Permission: Wagtail admin | admin | Can access Wagtail admin>, <Permission: Wagtail documents | document | Can add document>, <Permission: Wagtail documents | document | Can change document>, <Permission: Wagtail documents | document | Can choose document>, <Permission: Wagtail documents | document | Can delete document>, <Permission: Wagtail images | image | Can add image>, <Permission: Wagtail images | image | Can change image>, <Permission: Wagtail images | image | Can choose image>, <Permission: Wagtail images | image | Can delete image> Student <Permission: Wagtail admin | admin | Can access Wagtail admin>, <Permission: Website | report index | Can view report index>, <Permission: Website | report | Can add report>, <Permission: Website | report | Can change report>, <Permission: Website | report | Can delete report>, <Permission: Website | report | Can view report> As you can see, it would appear only admin access seems like the prerequisite for seeing content in the Admin interface. Despite this, logging in as Student still shows no changes in the Wagtail admin panel. And attempts to access any report in the admin interface (as you would as admin) continue to get a permission denied return code. What step am I missing here to ensure custom groups can access content in the admin interface?","['[[ANSWER_1], [VOTES=0]] : Page permissions in Wagtail are determined by position within the page hierarchy rather than by page type. So, while it\'s not possible to directly give students edit permission over the Report page model - if all your Report pages exist under a single ReportIndex (something that can be enforced through the use of parent_page_types / subpage_types rules), you can give them add/change permission over that ReportIndex page, which will thus allow them to create Report pages there. Page permissions are defined through the wagtail.models.GroupPagePermission model, which is distinct from Django\'s built-in group/permission relation. To set such a permission rule up, you can do the following: from wagtail.models import GroupPagePermission from django.contrib.auth.models import Permission, Group from myapp.models import ReportIndex students, created = Group.objects.get_or_create(name=""Student"") add_page = Permission.objects.get(codename=""add_reportpage"") report_index_page = ReportIndex.objects.first() GroupPagePermission.objects.create( group=students, page=report_index_page, permission=add_page, )']",1,"python, wagtail, wagtail-admin, python, wagtail, wagtail-admin",https://stackoverflow.com/questions/79155921/wagtail-cms-programmatically-enabling-user-access-to-manage-pages-within-the-a
51,"Pandas to excel, decimal columns are getting converted to text","df['some_col1'] = df['some_col1'].apply(lambda x: Decimal(x) if pd.notnull(x) else None) df['some_col2'] = df['some_col2'].apply(lambda x: Decimal(x) if pd.notnull(x) else None) output = io.BytesIO() with pd.ExcelWriter(output, engine='xlsxwriter') as writer: df.to_excel(writer, index=False, sheet_name='Sheet1') workbook = writer.book worksheet = writer.sheets['Sheet1'] # Define number format number_format = workbook.add_format({'num_format': '0.0000'}) # Apply formatting based on column name for col_num, col_name in enumerate(df.columns): if col_name == 'some_col1' or col_name == 'some_col2': # Specify by column name worksheet.set_column(col_num, col_num, None, number_format) It doesn't work even if I specify the columns directly in set_column (""A:A"" for example) Columns are decimals for sure, but they are in text format in Excel sheet: Thanks","[""[[ANSWER_1], [VOTES=3]] : Known issue. Here is the Github Issue link: link The issue is caused by the Pandas to_excel method where decimal.Decimal values are being saved as text is due to Pandas not recognising it as a numeric type. To fix this you can modify the decimals to float: df['some_col1'] = df['some_col1'].apply(lambda x: float(Decimal(x)) if pd.notnull(x) else None) df['some_col2'] = df['some_col2'].apply(lambda x: float(Decimal(x)) if pd.notnull(x) else None) This will be recognised as a numeric type and not be text in Excel."", ""[[ANSWER_2], [VOTES=0]] : Try changing the column type using: df['some_col1'] = pd.to_numeric(df['some_col1']).apply(lambda x: float(x) if pd.notnull(x) else None) df['some_col2'] = pd.to_numeric(df['some_col2']).apply(lambda x: float(x) if pd.notnull(x) else None)""]",2,"python, excel, pandas, python, excel, pandas",https://stackoverflow.com/questions/79155856/pandas-to-excel-decimal-columns-are-getting-converted-to-text
51,Find a mistake in algoritm [closed],"Closed. This question needs to be more focused. It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post. Closed 3 hours ago. Improve this question Ann and her dog Avi are choosing a time to travel to a new country. Ann thinks that her vacation was great if the temperature in the country became higher during the trip. Moreover, the higher it is at the time of departure relative to the day of arrival, the better. The weather forecast for some time ahead is given. You need to specify the temperature change for the best period for Dasha and Avi, as well as the best day numbers for arrival and departure. If you get several best options, specify the day numbers of the shortest trip with the closest departure date. def find_best_trip(temperatures): n = len(temperatures) max_change = 0 best_start = -1 best_end = -1 for start in range(n): for end in range(start + 1, n): change = temperatures[end] - temperatures[start] if change > max_change: max_change = change best_start = start best_end = end elif change == max_change: if (best_end - best_start) > (end - start) or (best_end - best_start) == ( end - start and start < best_start): best_start = start best_end = end return max_change, best_start, best_end temperatures = list(map(lambda x: int(x), input().split(' '))) result = find_best_trip(temperatures) print(result[0], result[1], result[2])",[],-8,"python, algorithm, python, algorithm",https://stackoverflow.com/questions/79155748/find-a-mistake-in-algoritm
51,How to convert unicode black pawn emoji to black pawn text character?,"I'm making chess in Python 3.12 using purely text for a challenge. The IDE I'm using is Visual Studio 2022. All the other unicode characters, including the white pawn, render as their text character variants fine, but the black pawn renders as an emoji by default. Upon research online, I've discovered this is because the black pawn unicode character as of Unicode 15 is the only one to have an emoji variant. How do I convert to the black pawn text character? Here is my code and the output (for context, liststringify is a function that just turns lists into strings, and the black pieces are white and the white pieces black because the ide uses a black background and uses white text): thecode What I tried was using the unicode text codes that are used to find the actual unicode characters, as VS2022 doesn't take just copy and pasting the actual unicode characters in well. I was expecting no emojis, as I was unaware of the black pawn emoji. I got the black pawn emoji.","['[[ANSWER_1], [VOTES=1]] : What gets displayed when there is ambiguity in available characters will depend on the combination of (1) The software you are using to display the characters (it can be as diverse as a terminal emulator program, for which there are tens of variants, a browser window, a text-editor like Microsoft Word) and (2) the font you are using, as configured in the settings or theme of that program. Unicode however DOES have a mechanism which can go inline with the text you want to display to force emoji or non emoji variants of the characters you want to display. The problem is that few programs will render correctly the sequence, even if the unicode sequence you send then is unambiguous. In this case, you should prefix the black pawn character (and preferably all the other chess piece characters, in order to be future-proof) with the U+FE0E (VARIATION SELECTOR-15) character - and, if everything is correctly implemented in the program you are using to display your characters (which, again, might not be the case), should normalize all pieces with their text variants. If that doesn\'t work, you can try different variations of the programs you are using to see your output, and, within these programs, configure different font-families, until you find a variation which enables you to work as you want. Actually it may all come down to a single font in your operating system which provides the Black Pawn as emoji - you could have success by simply uninstalling this font, if you can identify it, to have all pieces behaving uniformly across different programs. Other than that: it is really important when creating questions to stack overflow that the code (And usually the output and error messages) in your question are formatted as part of the question here. This specific case depends on the programming you are using and not on your code, so it was answerable - but typically on answering a question people will want to quote your code to create an improved version, and no one is willing to retype your code).} Also, taking a look at your code, although your question is not related to it: you would gain a lot, both in file maintainability, which includes ease to perform edits, and ease to write to, size and performance, just use a dictionary instead of this if/elif sequence you created there. A single dictionary mapping the pieces names to their character is trivial to write in Python as: `pieces = {""WKing"": ""\\u2654"", ...} And then, after the dicionary with as few as 14 entries, you can just do phile.append(f"" \\ufe0e{pieces[gameboard[i][e]} "") without a single if statement needed.']",2,"python, python-3.x, visual-studio-2022, python-unicode, unicode-string, python, python-3.x, visual-studio-2022, python-unicode, unicode-string",https://stackoverflow.com/questions/79155745/how-to-convert-unicode-black-pawn-emoji-to-black-pawn-text-character
51,Join differently nested lists in polars columns,"As you might have recognized from my other questions I am transitioning from pandas to polars right now. I have a polars df with differently nested lists like this: ┌────────────────────────────────────┬────────────────────────────────────┬─────────────────┬──────┐ │ col1 ┆ col2 ┆ col3 ┆ col4 │ │ --- ┆ --- ┆ --- ┆ --- │ │ list[list[str]] ┆ list[list[str]] ┆ list[str] ┆ str │ ╞════════════════════════════════════╪════════════════════════════════════╪═════════════════╪══════╡ │ [[""a"", ""a""], [""b"", ""b""], [""c"", ""c""]┆ [[""a"", ""a""], [""b"", ""b""], [""c"", ""c""]┆ [""A"", ""B"", ""C""] ┆ 1 │ │ [[""a"", ""a""]] ┆ [[""a"", ""a""]] ┆ [""A""] ┆ 2 │ │ [[""b"", ""b""], [""c"", ""c""]] ┆ [[""b"", ""b""], [""c"", ""c""]] ┆ [""B"", ""C""] ┆ 3 │ └────────────────────────────────────┴────────────────────────────────────┴─────────────────┴──────┘ Now I want to join the lists inside out using different separators to reach this: ┌─────────────┬─────────────┬───────┬──────┐ │ col1 ┆ col2 ┆ col3 ┆ col4 │ │ --- ┆ --- ┆ --- ┆ --- │ │ str ┆ str ┆ str ┆ str │ ╞═════════════╪═════════════╪═══════╪══════╡ │ a+a-b+b-c+c ┆ a+a-b+b-c+c ┆ A-B-C ┆ 1 │ │ a+a ┆ a+a ┆ A ┆ 2 │ │ b+b-c+c ┆ b+b-c+c ┆ B-C ┆ 3 │ └─────────────┴─────────────┴───────┴──────┘ I do this by using map_elements and a for loop, but I guess that is highly inefficient. Is there a polars native way to manage this? Here is my code: import polars as pl df = pl.DataFrame({""col1"": [[[""a"", ""a""], [""b"", ""b""], [""c"", ""c""]], [[""a"", ""a""]], [[""b"", ""b""], [""c"", ""c""]]], ""col2"": [[[""a"", ""a""], [""b"", ""b""], [""c"", ""c""]], [[""a"", ""a""]], [[""b"", ""b""], [""c"", ""c""]]], ""col3"": [[""A"", ""B"", ""C""], [""A""], [""B"", ""C""]], ""col4"": [""1"", ""2"", ""3""]}) nested_list_cols = [""col1"", ""col2""] list_cols = [""col3""] for col in nested_list_cols: df = df.with_columns(pl.lit(df[col].map_elements(lambda listed: ['+'.join(element) for element in listed], return_dtype=pl.List(pl.String))).alias(col)) # is the return_dtype always pl.List(pl.String)? for col in list_cols + nested_list_cols: df = df.with_columns(pl.lit(df[col].list.join(separator='-')).alias(col))","['[[ANSWER_1], [VOTES=0]] : pl.Expr.list.eval() to go over inner lists. pl.Expr.list.join() to join lists. df.with_columns( pl.col(""col1"",""col2"").list.eval(pl.element().list.join(""+"")).list.join(""-""), pl.col(""col3"").list.join(""-"") ) shape: (3, 4) ┌─────────────┬─────────────┬───────┬──────┐ │ col1 ┆ col2 ┆ col3 ┆ col4 │ │ --- ┆ --- ┆ --- ┆ --- │ │ str ┆ str ┆ str ┆ i64 │ ╞═════════════╪═════════════╪═══════╪══════╡ │ a+a-b+b-c+c ┆ a+a-b+b-c+c ┆ A-B-C ┆ 1 │ │ a+a ┆ a+a ┆ A ┆ 2 │ │ b+b-c+c ┆ b+b-c+c ┆ B-C ┆ 3 │ └─────────────┴─────────────┴───────┴──────┘', '[[ANSWER_2], [VOTES=0]] : You could use list.eval() and .list.join() df.with_columns( pl.col(nested_list_cols).list.eval(pl.element().list.join(""+"")).list.join(""-""), pl.col(list_cols).list.join(""-"") ) shape: (3, 4) ┌─────────────┬─────────────┬───────┬──────┐ │ col1 ┆ col2 ┆ col3 ┆ col4 │ │ --- ┆ --- ┆ --- ┆ --- │ │ str ┆ str ┆ str ┆ str │ ╞═════════════╪═════════════╪═══════╪══════╡ │ a+a-b+b-c+c ┆ a+a-b+b-c+c ┆ A-B-C ┆ 1 │ │ a+a ┆ a+a ┆ A ┆ 2 │ │ b+b-c+c ┆ b+b-c+c ┆ B-C ┆ 3 │ └─────────────┴─────────────┴───────┴──────┘']",2,"python, python-polars, python, python-polars",https://stackoverflow.com/questions/79155737/join-differently-nested-lists-in-polars-columns
51,How i can dispose a widget in pyqt6?,"Im trying to dispose a widget of mine its inside another class as a different component ,i want to dispose it in order to re-render the state of some of my buttons. Here is the class i want to re-render : class PlotScreen(QWidget): def __init__(self): super().__init__() self.paused = False # My button State And here i call my main class where it have a header with a button for navigation , i want when i navigate back to my home page to trigger an event or a dispose method to re-render the entire widget PlotScreen from the start (sweep_button_screen renders the PlotScreen). class MenuApp(QWidget): def __init__(self): super().__init__() def create_screens(self): dashboard_screen = QWidget() dashboard_layout = QVBoxLayout(dashboard_screen) sweep_button = QPushButton(""Sweep"") sweep_button.clicked.connect(lambda: self.navigate_to_screen(1)) dashboard_layout.addStretch(1) dashboard_layout.addWidget( sweep_button, alignment=Qt.AlignmentFlag.AlignHCenter ) self.stacked_widget.addWidget(dashboard_screen) # Index 0 (Home Page) self.stacked_widget.addWidget(sweep_button_screen) # Index 1 def navigate_to_screen(self, index): self.stacked_widget.setCurrentIndex(index) def navigate_to_home(self): self.stacked_widget.setCurrentIndex(0)",[],0,"python, pyqt, pyqt6, python, pyqt, pyqt6",https://stackoverflow.com/questions/79155725/how-i-can-dispose-a-widget-in-pyqt6
51,How to pass TVP data to a sql query in python (pyodbc),"I am trying to pass a Table valued parameter from python to MSSQL query. This is my table value that i want pass as a param. table_values = [ 'table_type_name', 'dbo', (1,None,'Test') ] Here is my param syntax. params = [(table_values,None)] Here i am executing this with pyodbc connection execute(SAVE_QUERY, params) here is my SQL query SAVE_QUERY= """""" SET NOCOUNT ON; SET ANSI_NULLS ON; DECLARE @table_values TABLE = ?; UPDATE table_name SET contacts.is_active = 0, FROM table_name LEFT JOIN @table_values CC ON CC.id = contacts.id WHERE CC.email IS NULL; """""" it is giving me the below error [42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Incorrect syntax near '='. (102) (SQLExecDirectW); [42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Statement(s) could not be prepared. (8180) What's the best way to receive the TVP data in this query and store it in variable for using it later part of query. I have tried removing DECLARE statement but its not working. What's the best way to receive the TVP data in this query and store it in variable for using it later part of query.",[],0,"python, sql-server, pyodbc, table-valued-parameters, python, sql-server, pyodbc, table-valued-parameters",https://stackoverflow.com/questions/79155720/how-to-pass-tvp-data-to-a-sql-query-in-python-pyodbc
51,"Robot Listener, listener v2 on keyword fail status generates a lot of screenshots instead of one","I used Robot Framework listener V2 to catch the current and capture screen shot if the status of the current keyword is FAIL. So my code is something like this class RobotListenerV2: ROBOT_LISTENER_API_VERSION = 2 ROBOT_LIBRARY_SCOPE = ""GLOBAL"" def end_keyword(self, name, attrs): # print('-------------------------------------------------------') # print('END keyword NAME => {} and ATTRIBUTES => {}'.format(name, attrs)) #print('END keyword NAME => {} and ATTRIBUTES => {}'.format(name, attrs['status'])) # print('-------------------------------------------------------') if attrs['status'] == 'FAIL' and attrs['status'] != 'NOT RUN': current_directory = os.getcwd() try: image_folder = '' variables = BuiltIn().get_variables() for k, v in variables.items(): if 'OUTPUT_DIR' in k: image_folder = v time_ = round(time.time() * 1000) if ' ' in image_folder: image_folder = image_folder.replace(' ','') if not os.path.exists(os.path.join(current_directory, image_folder)): os.makedirs(os.path.join(current_directory, image_folder)) image_name = self.full_screen(image_folder) url = self.get_web_driver_instance().current_url data_uri = base64.b64encode(open(image_name, 'rb').read()).decode('utf-8') logger.info( '<tr><td colspan=""3"">' '<a ' f'href=""data:image/png;base64,{data_uri}"">' '<img ' f'src=""data:image/png;base64,{data_uri}"" width=""{800}px"">' '</a></td></tr>', html=True, ) logger.info('<td class=""message""><a href=""{}"">""{}""</a></td>'.format(url, url), html=True) logger.info('<td class=""message"">Keyword Name : {} and attributes : {}</td>'.format(name, attrs), html=True) except Exception as ex: print(""We have Exception:"", type(ex)) print(""We have Exception:"", repr(ex)) if hasattr(ex, 'message'): print(ex.message) else: print(ex) finally: What I found is that Robot Framework takes a lot of screenshot starting from the current keyword until his parent keyword. But what I want is only one screenshot for the current keyword. Any Idea on how to restrict taking screenshot for the only failing keyword? If you need an extra details I will put a complete example",[],0,"python, selenium-webdriver, robotframework, listener, python, selenium-webdriver, robotframework, listener",https://stackoverflow.com/questions/79155702/robot-listener-listener-v2-on-keyword-fail-status-generates-a-lot-of-screenshot
51,How to Speed Up Document Retrieval with llama_index Using a Local Model in Jupyter Notebook?,"I'm working on a project that uses llama_index to retrieve document information in Jupyter Notebook, but I'm experiencing very slow query response times (around 15 minutes per query). I'm using the following code: from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings from llama_index.embeddings.huggingface import HuggingFaceEmbedding from llama_index.llms.ollama import Ollama documents = SimpleDirectoryReader(""C:path/example/data"").load_data() # Using bge-base embedding model Settings.embed_model = HuggingFaceEmbedding(model_name=""BAAI/bge-base-en-v1.5"") # Setting up Ollama LLM with a timeout of 1 hour Settings.llm = Ollama(model=""llama3"", request_timeout=3600.0) index = VectorStoreIndex.from_documents(documents) query_engine = index.as_query_engine() response = query_engine.query(""What did the author do growing up?"") print(response) I’m running this on a localhost Jupyter Notebook, and it consistently takes 15 minutes or longer to return results. Reducing request_timeout to speed up the query, but it results in a ReadTimeout error from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings from llama_index.embeddings.huggingface import HuggingFaceEmbedding from llama_index.llms.ollama import Ollama documents = SimpleDirectoryReader(""C:path/example/data"").load_data() # Using bge-base embedding model Settings.embed_model = HuggingFaceEmbedding(model_name=""BAAI/bge-base-en-v1.5"") # Setting up Ollama LLM with a timeout of 1 hour Settings.llm = Ollama(model=""llama3"", request_timeout=60.0) index = VectorStoreIndex.from_documents(documents) query_engine = index.as_query_engine() response = query_engine.query(""What did the author do growing up?"") print(response) How can I speed up the response time when querying documents? Are there ways to optimize or fully use a local model to improve retrieval speed? Specifically, is there a way to handle embeddings and LLM processing locally to avoid network latency or timeouts? Any help on reducing retrieval time or configuring a local model setup",[],0,"python, large-language-model, llama-index, ollama, llama3, python, large-language-model, llama-index, ollama, llama3",https://stackoverflow.com/questions/79155696/how-to-speed-up-document-retrieval-with-llama-index-using-a-local-model-in-jupyt
51,"Python - Tkinter App, Issues when calling scripts with subprocess","I have the following function which is used inside a Tkinter GUI app, for starting different other scripts (each with their own GUI's and functions) as subprocesses. def run_external_script(script_center_root, script_name, *args): script_center_root.withdraw() script_center_root.protocol(""WM_DELETE_WINDOW"", lambda: None) # Disable close button base_path = getattr(sys, '_MEIPASS', os.path.dirname(os.path.abspath(__file__))) script_path = os.path.join(base_path, script_name) try: print(""process started"") process = subprocess.Popen([python_executable, script_path, *args], shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, stdin=subprocess.PIPE ) #process.wait() print(""process stopped"") except subprocess.CalledProcessError as e: print(f""Error: Subprocess for '{script_name}' failed with exit code {e.returncode}"") print(f""Standard Output:\n{e.stdout}"") print(f""Error Output:\n{e.stderr}"") messagebox.showerror(""Error"", f""Could not run script: {script_name}\n{e.stderr}"") finally: script_center_root.protocol(""WM_DELETE_WINDOW"", lambda: close_window(script_center_root)) # Re-enable close button reopen_main_window(script_center_root) When I run it directly from PyCharm it works perfectly. When I create an exe using PyInstaller it doesn't open the scripts. This is the .spec file a = Analysis( ['main.py'], pathex=['.'], binaries=[], datas=[ ('XML_to_xlsx.py', '.'), ('xlsx_to_PIES.py', '.'), ('xlsx_to_ACES.py', '.'), ('aces_xlsx_expand.py', '.'), ('aces_xlsx_rollup.py', '.'), ('transpose_xlsx.py', '.') ], hiddenimports=['openpyxl','pandas', 'lxml'], hookspath=[], hooksconfig={}, runtime_hooks=[], excludes=[], noarchive=False, optimize=0, ) pyz = PYZ(a.pure) exe = EXE( pyz, a.scripts, a.binaries, a.zipfiles, a.datas, [], name='main', debug=False, bootloader_ignore_signals=False, strip=False, upx=True, console=True, disable_windowed_traceback=False, argv_emulation=False, target_arch=None, codesign_identity=None, entitlements_file=None, ) coll = COLLECT( exe, a.scripts, a.binaries, a.datas, strip=False, upx=True, upx_exclude=[], name='main', ) I need some pointers or help getting it working. I tried debugging it, but the console is showing no errors",[],0,"python, tkinter, subprocess, python, tkinter, subprocess",https://stackoverflow.com/questions/79155693/python-tkinter-app-issues-when-calling-scripts-with-subprocess
51,selenium python to create auto_login,"I have element in loginpage like this <input ng-model=""$ctrl.username"" id=""loginUserName"" class=""form-control ng-valid ng-not-empty ng-dirty ng-valid-parse ng-touched"" autocapitalize=""off"" autocorrect=""off"" autocomplete=""off"" sw-autofocus="""" style=""""> tried to store in el by el = driver.find_element(By.XPATH,'//*[@id=""loginUserName""]') got error message NoSuchWindowException: Message: no such window: target window already closed from unknown error: web view not found (Session info: chrome=130.0.6723.92) Stacktrace: GetHandleVerifier [0x00007FF68EF43AF5+28005] (No symbol) [0x00007FF68EEA83F0] (No symbol) [0x00007FF68ED4580A] (No symbol) [0x00007FF68ED1FA85] (No symbol) [0x00007FF68EDC2AD7] (No symbol) [0x00007FF68EDDB1B1] (No symbol) [0x00007FF68EDBB7E3] (No symbol) [0x00007FF68ED875C8] (No symbol) [0x00007FF68ED88731] GetHandleVerifier [0x00007FF68F23646D+3118813] GetHandleVerifier [0x00007FF68F286CC0+3448624] GetHandleVerifier [0x00007FF68F27CF3D+3408301] GetHandleVerifier [0x00007FF68F00A44B+841403] (No symbol) [0x00007FF68EEB344F] (No symbol) [0x00007FF68EEAF4C4] (No symbol) [0x00007FF68EEAF65D] (No symbol) [0x00007FF68EE9EBB9] BaseThreadInitThunk [0x00007FFC1173257D+29] RtlUserThreadStart [0x00007FFC12BCAF08+40] what's wrong with the method or code?","['[[ANSWER_1], [VOTES=0]] : Based on the NoSuchWindowException error it is likely that your Selenium instance has lost connection with the browser window due to a reason that it unclear. Try these: Ensure the window is acitve Restart any driver sessions. Switch to the correct window/tab. driver.switch_to.window(driver.window_handles[-1]) I have also seen this issue previously where the cookies invalidate the session after several requests so you may try deleting them prior. driver.delete_all_cookies()']",0,"python, selenium-webdriver, python, selenium-webdriver",https://stackoverflow.com/questions/79155645/selenium-python-to-create-auto-login
51,IBM db2 - alembic migration,"I am trying to run alembic upgrade head on my IBM cloud DB2 lite database, but getting KeyError: ibm_db_sa My .venv already has installed the packages ibm_db, ibm_db_sa, and ibm_db_alembi, and the connection-url works when connecting to the DB via sqlalchemy (creating an engine / session). The detailed error tracing is below. (.venv) PS C:\Users\ryan4\GitHub\py-playground\cell-data-db\cell_data_db\db2> alembic upgrade head Traceback (most recent call last): File ""C:\Users\ryan4\AppData\Local\Programs\Python\Python310\lib\runpy.py"", line 196, in _run_module_as_main return _run_code(code, main_globals, None, File ""C:\Users\ryan4\AppData\Local\Programs\Python\Python310\lib\runpy.py"", line 86, in _run_code exec(code, run_globals) File ""C:\Users\ryan4\GitHub\py-playground\cell-data-db\.venv\Scripts\alembic.exe\__main__.py"", line 7, in <module> File ""C:\Users\ryan4\GitHub\py-playground\cell-data-db\.venv\lib\site-packages\alembic\config.py"", line 636, in main CommandLine(prog=prog).main(argv=argv) File ""C:\Users\ryan4\GitHub\py-playground\cell-data-db\.venv\lib\site-packages\alembic\config.py"", line 626, in main self.run_cmd(cfg, options) File ""C:\Users\ryan4\GitHub\py-playground\cell-data-db\.venv\lib\site-packages\alembic\config.py"", line 603, in run_cmd fn( File ""C:\Users\ryan4\GitHub\py-playground\cell-data-db\.venv\lib\site-packages\alembic\command.py"", line 406, in upgrade script.run_env() File ""C:\Users\ryan4\GitHub\py-playground\cell-data-db\.venv\lib\site-packages\alembic\script\base.py"", line 586, in run_env util.load_python_file(self.dir, ""env.py"") File ""C:\Users\ryan4\GitHub\py-playground\cell-data-db\.venv\lib\site-packages\alembic\util\pyfiles.py"", line 95, in load_python_file module = load_module_py(module_id, path) File ""C:\Users\ryan4\GitHub\py-playground\cell-data-db\.venv\lib\site-packages\alembic\util\pyfiles.py"", line 113, in load_module_py spec.loader.exec_module(module) # type: ignore File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed File ""C:\Users\ryan4\GitHub\py-playground\cell-data-db\cell_data_db\db2\alembic\env.py"", line 82, in <module> run_migrations_online() File ""C:\Users\ryan4\GitHub\py-playground\cell-data-db\cell_data_db\db2\alembic\env.py"", line 71, in run_migrations_online context.configure( File ""<string>"", line 8, in configure File ""C:\Users\ryan4\GitHub\py-playground\cell-data-db\.venv\lib\site-packages\alembic\runtime\environment.py"", line 915, in configure self._migration_context = MigrationContext.configure( File ""C:\Users\ryan4\GitHub\py-playground\cell-data-db\.venv\lib\site-packages\alembic\runtime\migration.py"", line 281, in configure return MigrationContext(dialect, connection, opts, environment_context) File ""C:\Users\ryan4\GitHub\py-playground\cell-data-db\.venv\lib\site-packages\alembic\runtime\migration.py"", line 207, in __init__ self.impl = ddl.DefaultImpl.get_by_dialect(dialect)( File ""C:\Users\ryan4\GitHub\py-playground\cell-data-db\.venv\lib\site-packages\alembic\ddl\impl.py"", line 132, in get_by_dialect return _impls[dialect.name] KeyError: 'ibm_db_sa'",[],0,"python, sqlalchemy, db2, alembic, python, sqlalchemy, db2, alembic",https://stackoverflow.com/questions/79155637/ibm-db2-alembic-migration
51,Conflicts loading ArcPy and arcgisbinding packages simultaneously,"After working fine for the last couple of months I have encoutered a problem when loading the ArcPy and arcgisbinding packages in R simultaneously. The inital error message was returned when I ran arcpy$CheckOutExtension(""Cartography"") while the arcgisbinding package was loaded by a function run earlier. This is said error: C:\Users\LEON~1.THO\CONDA~1\envs\r-arcpy\lib\site-packages\numpy\__init__.py:143: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service from . import _distributor_init List of 22 $ python : chr ""C:/Users/leon.thoma/.conda/envs/r-arcpy/python.exe"" $ libpython : chr ""C:/Users/leon.thoma/.conda/envs/r-arcpy/python39.dll"" $ pythonhome : chr ""C:/Users/leon.thoma/.conda/envs/r-arcpy"" $ pythonpath : chr ""C:\\Program Files\\ArcGIS\\Pro\\Resources\\ArcPy;C:\\Users\\LEON~1.THO\\CONDA~1\\envs\\r-arcpy\\python39.zip;C:""| __truncated__ $ prefix : chr ""C:\\Users\\LEON~1.THO\\CONDA~1\\envs\\r-arcpy"" $ exec_prefix : chr ""C:\\Users\\LEON~1.THO\\CONDA~1\\envs\\r-arcpy"" $ base_exec_prefix : chr ""C:\\Users\\LEON~1.THO\\CONDA~1\\envs\\r-arcpy"" $ virtualenv : chr """" $ virtualenv_activate : chr """" $ executable : chr ""C:\\Users\\LEON~1.THO\\CONDA~1\\envs\\r-arcpy\\python.exe"" $ base_executable : chr ""C:\\Users\\LEON~1.THO\\CONDA~1\\envs\\r-arcpy\\python.exe"" $ version_string : chr ""3.9.18 [MSC v.1931 64 bit (AMD64)]"" $ version : chr ""3.9"" $ architecture : chr ""64bit"" $ anaconda : logi FALSE $ conda : chr ""True"" $ numpy :List of 2 ..$ path : chr ""C:/Users/leon.thoma/.conda/envs/r-arcpy/Lib/site-packages/numpy"" ..$ version:Class 'numeric_version' hidden list of 1 .. ..$ : int [1:3] 1 20 1 $ required_module : chr ""arcgis"" $ required_module_path: chr ""C:\\Users\\LEON~1.THO\\CONDA~1\\envs\\r-arcpy\\lib\\site-packages\\arcgis\\__init__.p"" $ available : logi TRUE $ python_versions : chr ""C:/Users/leon.thoma/.conda/envs/r-arcpy/python.exe"" $ forced : chr ""import(\""arcgis\"")"" - attr(*, ""class"")= chr ""py_config"" Error in `map()`: ℹ In index: 1. Caused by error in `map()`: ℹ In index: 1. Caused by error: ! Python module arcpy was not found. Detected Python configuration: Following the suggestion on (re)installing the mkl-service package from the given GitHub page yielded: ## Using --user flag workaround when installing mkl-service due to permission restrictions python -m pip install --user mkl-service Requirement already satisfied: mkl-service in c:\programdata\anaconda3\lib\site-packages (2.3.0) Requirement already satisfied: six in c:\programdata\anaconda3\lib\site-packages (from mkl-service) (1.15.0) Running either of the functions that are loading Arcpy or arcgisbinding seperately works as intended. Changing the order in which the packages are loaded, results in an error from whichever package was loaded last. When loading the ArcPy package first and then running arcgisbinding::arc.check_product() the following error is returned: Error: Could not bind to a valid ArcGIS Pro installation.` Running reticulate::py_config() returns: python: C:/Users/leon.thoma/.conda/envs/r-arcpy/python.exe libpython: C:/Users/leon.thoma/.conda/envs/r-arcpy/python39.dll pythonhome: C:/Users/leon.thoma/.conda/envs/r-arcpy version: 3.9.18 [MSC v.1931 64 bit (AMD64)] Architecture: 64bit numpy: C:/Users/leon.thoma/.conda/envs/r-arcpy/Lib/site-packages/numpy numpy_version: 1.20.1 arcgis: C:\Users\LEON~1.THO\CONDA~1\envs\r-arcpy\lib\site-packages\arcgis\__init__.p NOTE: Python version was forced by use_python() function The above is consistent for all cases, i.e. only loading arcgisbinding, loading arcgisbinding then arcpy, only arcpy, arcpy then arcgisbinding. I don't know how to dive deeper into what causes the issue. Any suggestions are welcome. Additional info: Running ArcGis Pro 3.1. arcgisbinding version 1.0.1.306 Session info of a fresh R session. R version 4.3.1 (2023-06-16 ucrt) Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 10 x64 (build 19045) Matrix products: default locale: [1] LC_COLLATE=German_Germany.utf8 LC_CTYPE=German_Germany.utf8 LC_MONETARY=German_Germany.utf8 [4] LC_NUMERIC=C LC_TIME=German_Germany.utf8 time zone: Europe/Berlin tzcode source: internal attached base packages: [1] stats graphics grDevices utils datasets methods base loaded via a namespace (and not attached): [1] compiler_4.3.1 tools_4.3.1 rstudioapi_0.16.0 Default Python interpreter in R-Studio is set to use the automatically build conda environment called 'r-arcpy' using Python 3.9.18",[],1,"python, r, arcgis, arcpy, python, r, arcgis, arcpy",https://stackoverflow.com/questions/79155635/conflicts-loading-arcpy-and-arcgisbinding-packages-simultaneously
51,Can't set value of input element using Selenium,"I'm trying to update the value of an input element using python and selenium webdriver. This is on Windows 10 using Edge. The page loads normally, and I can interact with the element using keyboard & mouse, but when I try to set the value using code I get the ""element not interactable"" error. Code: from selenium import webdriver from selenium.webdriver.common.by import By edge_service = webdriver.EdgeService(executable_path=edge_driver_path) edge_driver = webdriver.Edge(service=edge_service) edge_driver.get(url) time.sleep(15) elements, macroponent_bs = edge_driver.find_elements(by=By.XPATH, value=""//*""), None for element in elements: if element.tag_name.startswith(""macroponent""): macroponent_bs = element.tag_name break if macroponent_bs: script = ""return document.querySelector('"" + macroponent_bs + ""')"" + \ "".shadowRoot.querySelector('iframe')"" iframe = edge_driver.execute_script(script=script) edge_driver.switch_to.frame(iframe) element = edge_driver.find_element(by=By.ID, value=element_id) print(f""Element value is >{element.get_attribute('value')}<"") print(f""Element is_enabled, is_displayed = {element.is_enabled()}, {element.is_displayed()}"") element.send_keys(""Hello"") Output: DevTools listening on ws://127.0.0.1:65277/devtools/browser/5c5b2c22-eade-45c7-a8ee-2b234dae3f76 [38728:37308:1104/075027.679:ERROR:fallback_task_provider.cc(127)] Every renderer should have at least one task provided by a primary task provider. If a ""Renderer"" fallback task is shown, it is a bug. If you have repro steps, please file a new bug and tag it as a dependency of crbug.com/739782. [38728:37308:1104/075032.464:ERROR:fallback_task_provider.cc(127)] Every renderer should have at least one task provided by a primary task provider. If a ""Renderer"" fallback task is shown, it is a bug. If you have repro steps, please file a new bug and tag it as a dependency of crbug.com/739782. Element value is >< Element is_enabled, is_displayed = True, False Error from send_keys: Exception has occurred: ElementNotInteractableException (note: full exception trace is shown but execution is paused at: on_btn_debug) Message: element not interactable (Session info: MicrosoftEdge=130.0.2849.68)","['[[ANSWER_1], [VOTES=0]] : There is too many problems to write a proper answer, but here are some tips: Use WebDriverWait: For waiting the page to load: from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.common.exceptions import TimeoutException try: WebDriverWait(driver, timeout).until( lambda d: d.execute_script(""return document.readyState"") == ""complete"" ) print(""Page has fully loaded."") except TimeoutException: print(""Timed out waiting for page to load."") For waiting that an element is available: from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.webdriver.common.by import By try: element = WebDriverWait(driver, timeout).until( EC.visibility_of_element_located((By.ID, value)) ) print(""Element is visible."") return element except TimeoutException: print(""Timed out waiting for the element to be visible."") return None If you want a more precise answer, please add an url. You could also use a debugger.']",0,"python, selenium-webdriver, python, selenium-webdriver",https://stackoverflow.com/questions/79155604/cant-set-value-of-input-element-using-selenium
51,Multimodal LLM Memory,"I am trying to add memory to my application. It is a multimodal Rag based system. def run_final_query(llm, query, base64_image=None, compressed_image_str=None): # Define the system prompt to set model behavior system_prompt = { ""role"": ""system"", ""content"": ""You are a helpful assistant"" } # Prepare the primary query content user_content = [{""type"": ""text"", ""text"": query or ""Here is the image context you provided:""}] # Conditionally add image content to user input if base64_image: user_content.append({ ""type"": ""image_url"", ""image_url"": {""url"": f""data:image/jpeg;base64,{base64_image}""} }) if compressed_image_str: user_content.append({ ""type"": ""image_url"", ""image_url"": {""url"": f""data:image/jpeg;base64,{compressed_image_str}""} }) # Create message structure messages = [ system_prompt, {""role"": ""user"", ""content"": user_content} ] # Stream the response from the LLM and print each chunk for chunk in llm.stream(messages): print(chunk.content, end="""", flush=True) print() I would like to add memory so that the model has context for all future answers. My rag returns an image compressed_image_str. This is the context to information in the documents needed to answer questions. base64_image is an image that the user uploads and can ask questions on it. Here is the calling function def answer_query(query="""", image_data=None, k=4): # Initialize ChatOpenAI model for image summarization if an image is provided llm = ChatOpenAI(model=""gpt-4o"", temperature=0) search_query = """" base64_image="""" if image_data!= None: # Generate the image summary base64_image = compress_image_from_pil(image_data) image_summary = generate_image_summary(llm, base64_image) search_query = image_summary # Append text query to the search query search_query = search_query + "" "" + query # Perform the Byaldi search based on the generated summary or text query search_results = model.search(search_query, k=k) # Decode images from search results and combine them vertically images = [decode_base64_image(result['base64']) for result in search_results] combined_image = combine_images_grid(images) if images else None # Compress the combined image for ChatGPT model usage compressed_image_str = compress_image_from_pil(combined_image) if combined_image else None # Prepare the final query and get the output run_final_query( llm=llm, query=query, base64_image=base64_image, compressed_image_str=compressed_image_str ) I am struggling to add memory so that the model has passed question context as well as the passed image the user uploaded to answer questions on.",[],0,"python, openai-api, langchain, large-language-model, multimodal, python, openai-api, langchain, large-language-model, multimodal",https://stackoverflow.com/questions/79155547/multimodal-llm-memory
51,Optimising log-linear learning implementation for multiple players in Python,"I am working on testing log-linear learning for the class of potential games, and I want to test it with more than two players. I am having issues optimising the code. Currently, I am utilising the fact that log linear learning induces a Markov chain and the transitions can be explained with a transition matrix. However as the number of players increases the dimensions of the transition matrix increase rapidly making it impossible to use the dense numpy arrays to store the transition matrix. I have tried using the scipy.sparse matrices but the computation is very slow - the way the matrix is used is by multiplying stationary distribution with the transition matrix. I would like to make it feasible to test for multiple players and optimise the computation time of the code. Here is the formulation of the transition matrix with numpy. self.action_profiles = enumerate(np.array(list(product(np.arange(self.no_actions), repeat = self.no_players)))) self.potential = np.zeros((self.no_action_profiles, 1)) P = np.zeros([self.no_action_profiles, self.no_action_profiles]) for idx, profile in self.action_profiles: self.potential[idx] = self.potential_function(profile) for player_id in range(self.no_players): mask = np.arange(len(profile)) != player_id opponents_actions = profile[mask] # extract the opponents actions from the action profile utilities = np.array([self.utility_functions[player_id](i, opponents_actions) for i in range(self.no_actions)]) exp_values = np.exp(beta * utilities) p = exp_values/np.sum(exp_values) i = idx - profile[player_id]*self.no_actions**(self.no_players - 1 - player_id) stride = self.no_actions ** (self.no_players - 1 - player_id) P[idx, i: i + self.no_actions**(self.no_players - player_id) : stride] += 1/self.no_players*p self.P = P And with scipy. self.action_profiles = enumerate(np.array(list(product(np.arange(self.no_actions), repeat = self.no_players)))) self.potential = lil_matrix((self.no_action_profiles, 1)) # P = np.zeros([self.no_action_profiles, self.no_action_profiles]) P_row, P_col, P_data = [], [], [] for idx, profile in self.action_profiles: self.potential[idx] = self.potential_function(profile) for player_id in range(self.no_players): mask = np.arange(len(profile)) != player_id opponents_actions = profile[mask] # extract the opponents actions from the action profile utilities = np.array([self.utility_functions[player_id](i, opponents_actions) for i in range(self.no_actions)]) exp_values = np.exp(beta * utilities) p = exp_values/np.sum(exp_values) i = idx - profile[player_id]*self.no_actions**(self.no_players - 1 - player_id) stride = self.no_actions ** (self.no_players - 1 - player_id) for j, prob in enumerate(p): P_row.append(idx) P_col.append(i + j * stride) P_data.append(prob / self.no_players) P = coo_matrix((P_data, (P_row, P_col)), shape=(self.no_action_profiles, self.no_action_profiles)) self.P = P.tocsr() return self.P And they are to be used like this. P = self.gameSetup.formulate_transition_matrix(beta) mu0 = self.mu_matrix.copy() self.expected_value = np.zeros((int(self.max_iter), 1)) P = np.linalg.matrix_power(P, scale_factor) for i in range(self.max_iter): mu = mu0 @ P mu0 = mu self.expected_value[i] = mu @ self.gameSetup.potential self.expected_value = self.expected_value self.stationary = mu",[],0,"python, performance, machine-learning, sparse-matrix, game-theory, python, performance, machine-learning, sparse-matrix, game-theory",https://stackoverflow.com/questions/79155493/optimising-log-linear-learning-implementation-for-multiple-players-in-python
51,Django Haitian Creole,I want to make a Django Project in Haitian Creole I am trying to create a django project in Haitian Creole. I put LANGUAGE_CODE = 'ht' USE_I18N = True USE_L10N = True USE_TZ = True in the settings.py I had installed gettext and run the follow command in my project folder django-admin makemessages -l ht But everytime I run my server it returns an error. Can somebody help me with this please?,[],0,"python, django, language-translation, python, django, language-translation",https://stackoverflow.com/questions/79155491/django-haitian-creole
51,Is there a python (or other language) command-line implementation of MiraCast? [closed],"Closed. This question is seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. It does not meet Stack Overflow guidelines. It is not currently accepting answers. We don’t allow questions seeking recommendations for software libraries, tutorials, tools, books, or other off-site resources. You can edit the question so it can be answered with facts and citations. Closed 4 hours ago. Improve this question I am aware there is catt for the chromecast, and many python command line interfaces for interacting with airplay, but is there such a version for MiraCast? I wish to be able to scan for devices and send media etc much like you can do with Windows+K on windows, but from a command line or python program. Thanks in advance. Have researched and looked through github, can only find receivers or airplay/chromecast versions.",[],1,"python, windows, chromecast, airplay, miracast, python, windows, chromecast, airplay, miracast",https://stackoverflow.com/questions/79155433/is-there-a-python-or-other-language-command-line-implementation-of-miracast
51,How to submit nested test suites with plugin qase-pytest?,"I'm struggling to created nested suites when using qase-pytest plugin, hence in the docs, it can be use dot annotation to create the nested test suites but it doesn't works. I use the library from here https://github.com/qase-tms/qase-python/tree/main/qase-pytest decorator function from qase-pytest : @staticmethod def suite(title: str, description: str = None): """""" >>> @qase.suite(""Sign up"") >>> def test_example(): >>> pass :param title: a string with suite name. You can use dot notation to create nested suites. :param description: a string with suite description :return: pytest.mark instance """""" return pytest.mark.qase_suite(title=title, description=description) how i used it : from qase.pytest import qase @qase.suite(""First Suites.Second Suites.Third Suites"") @qase.title(""This is Example Title"") def test_example(): assert True Instead of giving me this : . └── Root Suites/ └── First Suites/ └── Second Suites/ └── Third Suites its create a Suites like this First Suites.Second Suites.Third Suites","['[[ANSWER_1], [VOTES=0]] : This is valid issue from qase-pytest plugin and it has been fixed in v.6.1.8. Reference : https://github.com/qase-tms/qase-python/issues/296']",0,"python, automation, automated-tests, pytest, testcase, python, automation, automated-tests, pytest, testcase",https://stackoverflow.com/questions/79155410/how-to-submit-nested-test-suites-with-plugin-qase-pytest
