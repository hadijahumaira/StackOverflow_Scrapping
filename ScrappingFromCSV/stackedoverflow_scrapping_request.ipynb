{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.split())  # Hapus newline dan gabungkan teks menjadi satu baris\n",
    "\n",
    "# Baca file CSV yang berisi link pertanyaan\n",
    "file_path = 'HasilCrawl_Request.csv'\n",
    "df_links = pd.read_csv(file_path)\n",
    "\n",
    "# List untuk menyimpan data detail pertanyaan\n",
    "questions_data = []\n",
    "\n",
    "for index, row in df_links.iterrows():\n",
    "    link = row['Link_question']\n",
    "    response = requests.get(link)\n",
    "    \n",
    "# Periksa apakah respons berhasil\n",
    "    if response.status_code == 200:\n",
    "        # Parse konten HTML dengan BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Loop melalui setiap blok pertanyaan\n",
    "        for area in soup.find_all('div', class_=\"s-post-summary js-post-summary\"):\n",
    "            title = area.find('a', class_=\"s-link\").get_text(strip=True)\n",
    "            question_excerpt = clean_text(area.find('div', class_=\"s-post-summary--content-excerpt\").get_text(strip=True))\n",
    "            votes = area.select_one('.s-post-summary--stats-item__emphasized .s-post-summary--stats-item-number').get_text(strip=True)\n",
    "            answers = area.select_one('.s-post-summary--stats-item:nth-child(2) .s-post-summary--stats-item-number').get_text(strip=True)\n",
    "            views = area.select_one('.s-post-summary--stats-item:nth-child(3) .s-post-summary--stats-item-number').get_text(strip=True)\n",
    "            tags = [tag.get_text(strip=True) for tag in area.select('.s-post-summary--meta-tags .post-tag')]\n",
    "            author = area.select_one('.s-user-card--info .flex--item').get_text(strip=True) if area.select_one('.s-user-card--info .flex--item') else 'Anonymous'\n",
    "            reputation = area.select_one('.s-user-card--rep').get_text(strip=True) if area.select_one('.s-user-card--rep') else '0'\n",
    "            link = \"https://stackoverflow.com\" + area.find('a', class_=\"s-link\")['href']\n",
    "\n",
    "            # Simpan data ke dalam dictionary\n",
    "            question_info = {\n",
    "                'Title': title,\n",
    "                'Excerpt': question_excerpt,\n",
    "                'Votes': votes,\n",
    "                'Answers': answers,\n",
    "                'Views': views,\n",
    "                'Tags': ', '.join(tags),\n",
    "                'Author': author,\n",
    "                'Reputation': reputation,\n",
    "                'Link': link\n",
    "            }\n",
    "\n",
    "            questions_data.append(question_info)\n",
    "\n",
    "# Simpan hasil scraping ke CSV  \n",
    "output_file = 'HasilScrapping_Request.csv'\n",
    "df_questions = pd.DataFrame(questions_data)\n",
    "df_questions.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini scraping dari link crawling yaaaaaaaaaaaaaaaaaa, yang request \n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Fungsi untuk membersihkan teks dari karakter newline dan whitespace berlebih\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.split())  # Hapus newline dan gabungkan teks menjadi satu baris\n",
    "\n",
    "# Baca file CSV yang berisi link pertanyaan\n",
    "file_path = 'stacked_huaaaareq.csv'\n",
    "df_links = pd.read_csv(file_path)\n",
    "\n",
    "# List untuk menyimpan data detail pertanyaan\n",
    "questions_data = []\n",
    "\n",
    "# Loop melalui setiap link pertanyaan\n",
    "for index, row in df_links.iterrows():\n",
    "    link = row['Link_question']\n",
    "    response = requests.get(link)\n",
    "    time.sleep(2)  # Tambahkan jeda untuk menghindari terlalu banyak permintaan sekaligus\n",
    "\n",
    "    # Periksa apakah respons berhasil\n",
    "    if response.status_code == 200:\n",
    "        # Parse konten HTML dengan BeautifulSoup\n",
    "        data = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Ambil informasi yang diperlukan dari setiap halaman\n",
    "        try:\n",
    "            title = clean_text(data.find('a', class_=\"question-hyperlink\").text)  # Judul pertanyaan\n",
    "            question_text = clean_text(data.find('div', class_=\"s-prose js-post-body\").text)  # Isi teks pertanyaan\n",
    "            votes = clean_text(data.find('div', class_=\"js-vote-count\").text)  # Jumlah vote\n",
    "            tags = [clean_text(tag.text) for tag in data.find_all('a', class_=\"post-tag\")]  # Tag pertanyaan\n",
    "\n",
    "            # Simpan data ke dalam dictionary\n",
    "            question_info = {\n",
    "                'Title': title,\n",
    "                'Question_Text': question_text,\n",
    "                'Votes': votes,\n",
    "                'Tags': ', '.join(tags),  # Gabungkan tag menjadi satu string\n",
    "                'Link': link\n",
    "            }\n",
    "            \n",
    "            questions_data.append(question_info)\n",
    "            \n",
    "            print(f\"Data berhasil diambil untuk pertanyaan ke-{index+1}: {title}\")\n",
    "        \n",
    "        except AttributeError:\n",
    "            print(f\"Gagal mengambil data untuk link: {link}\")\n",
    "    else:\n",
    "        print(f\"Permintaan gagal untuk link: {link} dengan status code {response.status_code}\")\n",
    "\n",
    "# Simpan hasil scraping ke CSV\n",
    "output_file = 'stacked_questions_details_req.csv'\n",
    "df_questions = pd.DataFrame(questions_data)\n",
    "df_questions.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Data detail pertanyaan berhasil disimpan ke\", output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
